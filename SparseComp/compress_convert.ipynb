{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Compress Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sparsecomp import compress_NN_models\n",
    "import torch\n",
    "from conversion import save_compressed_model\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_model(model):\n",
    "    assert isinstance(model, nn.Module), \"The model is not a subclass of torch.nn.Module\"\n",
    "    kb = 1000\n",
    "    model_size = 0\n",
    "    params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_size = param.nelement() * param.element_size()\n",
    "        model_size += layer_size\n",
    "        # print(name,\"\\t\", param.nelement(), \"\\t\", param.element_size(),\"\\t\", layer_size / kb, \"KB\")\n",
    "\n",
    "    for name, buffer in model.named_buffers():\n",
    "        layer_size = buffer.nelement() * buffer.element_size()\n",
    "        model_size += layer_size\n",
    "        # print(name,\"\\t\", layer_size / kb, \"KB\")\n",
    "    # print(\"Model Size:\", model_size / kb, \"KB\")\n",
    "\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    # print(\"Model Params:\", params)\n",
    "\n",
    "    return (model_size / kb), params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline_compressed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline_compressed, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"valid\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=25, bias=False),\n",
    "            nn.Linear(in_features=25, out_features=256)\n",
    "        )\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        exit_outputs = []\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        exit_outputs.append(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1[0](x)\n",
    "        x = self.fc1[1](x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x, exit_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"valid\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        exit_outputs = []\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        exit_outputs.append(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x, exit_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for S_earlyexit_model\n",
    "class GeneralEEModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneralEEModel, self).__init__()\n",
    "        self.pool_kernels = [\n",
    "            (1, 6, 6), (1, 3, 3), (1, 1, 1)\n",
    "        ]\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc = nn.Linear(in_features=1024, out_features=10)\n",
    "    \n",
    "    def forward(self, x, inference=False):\n",
    "        pooled_outs = []\n",
    "        for layer, out in enumerate(x):\n",
    "            pool_3d = nn.MaxPool3d(kernel_size=self.pool_kernels[layer])\n",
    "            pooled_outs.append(pool_3d(x))\n",
    "        x = torch.cat(pooled_outs, dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        scores = self.fc(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline-S Model Size: 929.32 KB\n",
      "Baseline-S Model Params: 232330\n",
      "Files already downloaded and verified\n",
      "Target size requested: 200 KB\n",
      "Iteration: 0\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 128, 13, 13]) 86528 295424 295424\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv2 with memory usage: 288.500 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 1\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv3 with memory usage: 288.250 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 2\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc1 with memory usage: 257.000 KB\n",
      "Sequential(\n",
      "  (0): Linear(in_features=256, out_features=25, bias=False)\n",
      "  (1): Linear(in_features=25, out_features=256, bias=True)\n",
      ")\n",
      "Iteration: 3\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 25]) 100 25600 25600\n",
      "torch.Size([1, 256]) 1024 26624 26624\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc2 with memory usage: 64.250 KB\n",
      "Layer already small 65792\n",
      "-------------- FINISHED FACTORIZATION --------------\n",
      "Test accuracy: 10.0\n",
      "----------------------------------------------------\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 1728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 16384, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 640, Zero Elements: 0\n",
      "Starting Density of model's parameters: [1, 1, 1, 1, 1, 1, 1]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Layer 1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 1:\t\t 7.168 KB, \tweight:\t 6.912 KB, \tbias: 0.256 KB\n",
      "Layer 2 Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 2:\t\t 295.424 KB, \tweight:\t 294.912 KB, \tbias: 0.512 KB\n",
      "Layer 3 Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 3:\t\t 295.168 KB, \tweight:\t 294.912 KB, \tbias: 0.256 KB\n",
      "Layer 4 Linear(in_features=256, out_features=25, bias=False)\n",
      "Layer 4:\t\t 25.6 KB, \tweight:\t 25.6 KB, \tbias: 0.0 KB\n",
      "Layer 5 Linear(in_features=25, out_features=256, bias=True)\n",
      "Layer 5:\t\t 26.624 KB, \tweight:\t 25.6 KB, \tbias: 1.024 KB\n",
      "Layer 6 Linear(in_features=256, out_features=64, bias=True)\n",
      "Layer 6:\t\t 65.792 KB, \tweight:\t 65.536 KB, \tbias: 0.256 KB\n",
      "Layer 7 Linear(in_features=64, out_features=10, bias=True)\n",
      "Layer 7:\t\t 2.6 KB, \tweight:\t 2.56 KB, \tbias: 0.04 KB\n",
      "Size FC Layer (no sparsity):\t 718.376 KB\n",
      "Size FC Layer (with sparsity):\t 718.376 KB\n",
      "Total Size no sparsity:\t\t 718.376 KB\n",
      "Total Size with sparsity:\t 718.376 KB\n",
      "Total Size with sparsity and CSC representation:\t 718.376 KB\n",
      "-------------------------------------------------------------------------------------------\n",
      "uncomp: 718.376 KB\n",
      "comp: 718.376 KB\n",
      "Starting size of the model: 718.376 KB\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [256, 25], strides() = [1, 256]\n",
      "param.sizes() = [256, 25], strides() = [25, 1] (Triggered internally at ../torch/csrc/autograd/functions/accumulate_grad.h:202.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step[100/352], Loss: 2.0317\n",
      "Epoch [1/10], Step[200/352], Loss: 1.8550\n",
      "Epoch [1/10], Step[300/352], Loss: 1.6844\n",
      "Epoch [1/10], Step[352/352], Loss: 1.5441\n",
      "Epoch[1]: v_loss: 1.56265 v_acc: 41.79\n",
      "Epoch [2/10], Step[100/352], Loss: 1.6095\n",
      "Epoch [2/10], Step[200/352], Loss: 1.4444\n",
      "Epoch [2/10], Step[300/352], Loss: 1.3523\n",
      "Epoch [2/10], Step[352/352], Loss: 1.3080\n",
      "Epoch[2]: v_loss: 1.39499 v_acc: 49.19\n",
      "Epoch [3/10], Step[100/352], Loss: 1.2748\n",
      "Epoch [3/10], Step[200/352], Loss: 1.4586\n",
      "Epoch [3/10], Step[300/352], Loss: 1.2893\n",
      "Epoch [3/10], Step[352/352], Loss: 1.3638\n",
      "Epoch[3]: v_loss: 1.30667 v_acc: 52.21\n",
      "Epoch [4/10], Step[100/352], Loss: 1.2313\n",
      "Epoch [4/10], Step[200/352], Loss: 1.1951\n",
      "Epoch [4/10], Step[300/352], Loss: 1.2303\n",
      "Epoch [4/10], Step[352/352], Loss: 1.0645\n",
      "Epoch[4]: v_loss: 1.24869 v_acc: 54.71\n",
      "Epoch [5/10], Step[100/352], Loss: 1.1482\n",
      "Epoch [5/10], Step[200/352], Loss: 1.0687\n",
      "Epoch [5/10], Step[300/352], Loss: 1.0948\n",
      "Epoch [5/10], Step[352/352], Loss: 1.1855\n",
      "Epoch[5]: v_loss: 1.16708 v_acc: 58.43\n",
      "Epoch [6/10], Step[100/352], Loss: 1.1660\n",
      "Epoch [6/10], Step[200/352], Loss: 1.1621\n",
      "Epoch [6/10], Step[300/352], Loss: 1.2848\n",
      "Epoch [6/10], Step[352/352], Loss: 1.1435\n",
      "Epoch[6]: v_loss: 1.13623 v_acc: 59.31\n",
      "Epoch [7/10], Step[100/352], Loss: 1.2226\n",
      "Epoch [7/10], Step[200/352], Loss: 1.2559\n",
      "Epoch [7/10], Step[300/352], Loss: 1.1173\n",
      "Epoch [7/10], Step[352/352], Loss: 1.0185\n",
      "Epoch[7]: v_loss: 1.1057 v_acc: 60.7\n",
      "Epoch [8/10], Step[100/352], Loss: 0.9299\n",
      "Epoch [8/10], Step[200/352], Loss: 1.2640\n",
      "Epoch [8/10], Step[300/352], Loss: 1.0150\n",
      "Epoch [8/10], Step[352/352], Loss: 0.8905\n",
      "Epoch[8]: v_loss: 1.09489 v_acc: 61.35\n",
      "Epoch [9/10], Step[100/352], Loss: 1.1049\n",
      "Epoch [9/10], Step[200/352], Loss: 1.0014\n",
      "Epoch [9/10], Step[300/352], Loss: 1.1167\n",
      "Epoch [9/10], Step[352/352], Loss: 1.0120\n",
      "Epoch[9]: v_loss: 1.11414 v_acc: 60.47\n",
      "Epoch [10/10], Step[100/352], Loss: 0.9679\n",
      "Epoch [10/10], Step[200/352], Loss: 0.8463\n",
      "Epoch [10/10], Step[300/352], Loss: 0.9835\n",
      "Epoch [10/10], Step[352/352], Loss: 1.0185\n",
      "Epoch[10]: v_loss: 1.0918 v_acc: 62.13\n",
      "Best model saved at epoch:  10\n",
      "Best acc model saved at epoch:  10\n",
      "Accuracy of the network on the 10000 test images: 62.13 %\n",
      "0 iteration -  Size: 452187.09907456 [1, 0.04782969, 1, 1, 1, 1, 1]\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n",
      "Epoch [1/10], Step[100/352], Loss: 1.0115\n",
      "Epoch [1/10], Step[200/352], Loss: 0.9507\n",
      "Epoch [1/10], Step[300/352], Loss: 1.0461\n",
      "Epoch [1/10], Step[352/352], Loss: 1.0856\n",
      "Epoch[1]: v_loss: 1.13214 v_acc: 59.96\n",
      "Epoch [2/10], Step[100/352], Loss: 1.0375\n",
      "Epoch [2/10], Step[200/352], Loss: 0.9661\n",
      "Epoch [2/10], Step[300/352], Loss: 1.0763\n",
      "Epoch [2/10], Step[352/352], Loss: 1.0290\n",
      "Epoch[2]: v_loss: 1.09675 v_acc: 61.31\n",
      "Epoch [3/10], Step[100/352], Loss: 0.9940\n",
      "Epoch [3/10], Step[200/352], Loss: 1.0899\n",
      "Epoch [3/10], Step[300/352], Loss: 0.8912\n",
      "Epoch [3/10], Step[352/352], Loss: 1.0016\n",
      "Epoch[3]: v_loss: 1.0684 v_acc: 62.05\n",
      "Epoch [4/10], Step[100/352], Loss: 1.1496\n",
      "Epoch [4/10], Step[200/352], Loss: 0.8775\n",
      "Epoch [4/10], Step[300/352], Loss: 0.8707\n",
      "Epoch [4/10], Step[352/352], Loss: 0.9396\n",
      "Epoch[4]: v_loss: 1.07427 v_acc: 62.14\n",
      "Epoch [5/10], Step[100/352], Loss: 0.8476\n",
      "Epoch [5/10], Step[200/352], Loss: 0.8802\n",
      "Epoch [5/10], Step[300/352], Loss: 1.0986\n",
      "Epoch [5/10], Step[352/352], Loss: 1.0104\n",
      "Epoch[5]: v_loss: 1.08769 v_acc: 61.58\n",
      "Epoch [6/10], Step[100/352], Loss: 0.9279\n",
      "Epoch [6/10], Step[200/352], Loss: 0.9669\n",
      "Epoch [6/10], Step[300/352], Loss: 1.0885\n",
      "Epoch [6/10], Step[352/352], Loss: 1.1316\n",
      "Epoch[6]: v_loss: 1.05469 v_acc: 63.0\n",
      "Epoch [7/10], Step[100/352], Loss: 0.8923\n",
      "Epoch [7/10], Step[200/352], Loss: 1.2644\n",
      "Epoch [7/10], Step[300/352], Loss: 1.0008\n",
      "Epoch [7/10], Step[352/352], Loss: 1.0539\n",
      "Epoch[7]: v_loss: 1.07375 v_acc: 62.06\n",
      "Epoch [8/10], Step[100/352], Loss: 0.9481\n",
      "Epoch [8/10], Step[200/352], Loss: 0.7816\n",
      "Epoch [8/10], Step[300/352], Loss: 0.9384\n",
      "Epoch [8/10], Step[352/352], Loss: 1.0123\n",
      "Epoch[8]: v_loss: 1.05543 v_acc: 62.92\n",
      "Epoch [9/10], Step[100/352], Loss: 0.9952\n",
      "Epoch [9/10], Step[200/352], Loss: 0.8364\n",
      "Epoch [9/10], Step[300/352], Loss: 1.0685\n",
      "Epoch [9/10], Step[352/352], Loss: 0.8375\n",
      "Epoch[9]: v_loss: 1.06923 v_acc: 62.4\n",
      "Epoch [10/10], Step[100/352], Loss: 0.8796\n",
      "Epoch [10/10], Step[200/352], Loss: 1.0576\n",
      "Epoch [10/10], Step[300/352], Loss: 1.0086\n",
      "Epoch [10/10], Step[352/352], Loss: 1.0226\n",
      "Epoch[10]: v_loss: 1.06573 v_acc: 62.64\n",
      "Best model saved at epoch:  6\n",
      "Best acc model saved at epoch:  6\n",
      "Accuracy of the network on the 10000 test images: 63.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "185742.19814912"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define your model (assuming GeneralEEModel is defined elsewhere)\n",
    "# model = GeneralEEModel().to(device)\n",
    "model = Baseline().to(device)\n",
    "model_size, params = print_full_model(model)\n",
    "print(\"Baseline-S Model Size:\", model_size, \"KB\")\n",
    "print(\"Baseline-S Model Params:\", params)\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "dataset = CIFAR10(root='./data', download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root='./data', train=False, transform=ToTensor())\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 128\n",
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, num_workers=4)\n",
    "\n",
    "# Define other parameters\n",
    "target_size = 512  # Target size in KB\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "regularizerParam = 0.0\n",
    "compressionStep = 0.1\n",
    "initialCompressionStep = 0.1\n",
    "fastCompression = False\n",
    "modelName = \"compressed_model\"\n",
    "device = \"cpu\"\n",
    "accuracyAware = True\n",
    "layersFactorization = True\n",
    "calculateInputs = None\n",
    "\n",
    "# Call the compress_NN_models function\n",
    "compress_NN_models(\n",
    "    model, target_size, train_loader, test_loader,\n",
    "    val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate,\n",
    "    criterion=criterion, regularizerParam=regularizerParam, compressionStep=compressionStep,\n",
    "    initialCompressionStep=initialCompressionStep, fastCompression=fastCompression,\n",
    "    modelName=modelName, device=device, accuracyAware=accuracyAware,\n",
    "    layersFactorization=layersFactorization, calculateInputs=calculateInputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Target size requested: 200 KB\n",
      "Iteration: 0\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 128, 13, 13]) 86528 295424 295424\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv2 with memory usage: 288.500 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 1\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv3 with memory usage: 288.250 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 2\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc1 with memory usage: 257.000 KB\n",
      "Sequential(\n",
      "  (0): Linear(in_features=256, out_features=25, bias=False)\n",
      "  (1): Linear(in_features=25, out_features=256, bias=True)\n",
      ")\n",
      "Iteration: 3\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 25]) 100 25600 25600\n",
      "torch.Size([1, 256]) 1024 26624 26624\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc2 with memory usage: 64.250 KB\n",
      "Layer already small 65792\n",
      "-------------- FINISHED FACTORIZATION --------------\n",
      "Test accuracy: 10.0\n",
      "----------------------------------------------------\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 1728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 16384, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 640, Zero Elements: 0\n",
      "Starting Density of model's parameters: [1, 1, 1, 1, 1, 1, 1]\n",
      "Starting size of the model: 718.376 KB\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n",
      "Epoch [1/10], Step[100/352], Loss: 2.0556\n",
      "Epoch [1/10], Step[200/352], Loss: 1.8050\n",
      "Epoch [1/10], Step[300/352], Loss: 1.7328\n",
      "Epoch [1/10], Step[352/352], Loss: 1.8614\n",
      "Epoch[1]: v_loss: 1.63954 v_acc: 38.8\n",
      "Epoch [2/10], Step[100/352], Loss: 1.5803\n",
      "Epoch [2/10], Step[200/352], Loss: 1.4922\n",
      "Epoch [2/10], Step[300/352], Loss: 1.5496\n",
      "Epoch [2/10], Step[352/352], Loss: 1.4441\n",
      "Epoch[2]: v_loss: 1.49792 v_acc: 44.0\n",
      "Epoch [3/10], Step[100/352], Loss: 1.4393\n",
      "Epoch [3/10], Step[200/352], Loss: 1.4913\n",
      "Epoch [3/10], Step[300/352], Loss: 1.3059\n",
      "Epoch [3/10], Step[352/352], Loss: 1.5887\n",
      "Epoch[3]: v_loss: 1.39102 v_acc: 48.43\n",
      "Epoch [4/10], Step[100/352], Loss: 1.4242\n",
      "Epoch [4/10], Step[200/352], Loss: 1.2871\n",
      "Epoch [4/10], Step[300/352], Loss: 1.3223\n",
      "Epoch [4/10], Step[352/352], Loss: 1.1258\n",
      "Epoch[4]: v_loss: 1.34912 v_acc: 50.44\n",
      "Epoch [5/10], Step[100/352], Loss: 1.4471\n",
      "Epoch [5/10], Step[200/352], Loss: 1.2930\n",
      "Epoch [5/10], Step[300/352], Loss: 1.3145\n",
      "Epoch [5/10], Step[352/352], Loss: 1.2659\n",
      "Epoch[5]: v_loss: 1.2565 v_acc: 53.84\n",
      "Epoch [6/10], Step[100/352], Loss: 0.9845\n",
      "Epoch [6/10], Step[200/352], Loss: 1.1710\n",
      "Epoch [6/10], Step[300/352], Loss: 1.1355\n",
      "Epoch [6/10], Step[352/352], Loss: 1.2812\n",
      "Epoch[6]: v_loss: 1.24036 v_acc: 54.67\n",
      "Epoch [7/10], Step[100/352], Loss: 1.1293\n",
      "Epoch [7/10], Step[200/352], Loss: 1.2322\n",
      "Epoch [7/10], Step[300/352], Loss: 1.0568\n",
      "Epoch [7/10], Step[352/352], Loss: 1.1905\n",
      "Epoch[7]: v_loss: 1.18402 v_acc: 57.43\n",
      "Epoch [8/10], Step[100/352], Loss: 1.2256\n",
      "Epoch [8/10], Step[200/352], Loss: 1.1091\n",
      "Epoch [8/10], Step[300/352], Loss: 1.1191\n",
      "Epoch [8/10], Step[352/352], Loss: 1.2661\n",
      "Epoch[8]: v_loss: 1.20069 v_acc: 56.58\n",
      "Epoch [9/10], Step[100/352], Loss: 1.1244\n",
      "Epoch [9/10], Step[200/352], Loss: 1.2998\n",
      "Epoch [9/10], Step[300/352], Loss: 1.1434\n",
      "Epoch [9/10], Step[352/352], Loss: 1.2101\n",
      "Epoch[9]: v_loss: 1.14524 v_acc: 59.34\n",
      "Epoch [10/10], Step[100/352], Loss: 1.0616\n",
      "Epoch [10/10], Step[200/352], Loss: 1.2180\n",
      "Epoch [10/10], Step[300/352], Loss: 1.2693\n",
      "Epoch [10/10], Step[352/352], Loss: 0.9335\n",
      "Epoch[10]: v_loss: 1.16423 v_acc: 57.94\n",
      "Best model saved at epoch:  9\n",
      "Best acc model saved at epoch:  9\n",
      "Accuracy of the network on the 10000 test images: 59.34 %\n",
      "0 iteration -  Size: 452187.09907456 [1, 0.04782969, 1, 1, 1, 1, 1]\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n",
      "Epoch [1/10], Step[100/352], Loss: 1.2743\n",
      "Epoch [1/10], Step[200/352], Loss: 1.1753\n",
      "Epoch [1/10], Step[300/352], Loss: 1.0715\n",
      "Epoch [1/10], Step[352/352], Loss: 1.2733\n",
      "Epoch[1]: v_loss: 1.19059 v_acc: 57.34\n",
      "Epoch [2/10], Step[100/352], Loss: 1.0868\n",
      "Epoch [2/10], Step[200/352], Loss: 1.0783\n",
      "Epoch [2/10], Step[300/352], Loss: 1.0607\n",
      "Epoch [2/10], Step[352/352], Loss: 0.9009\n",
      "Epoch[2]: v_loss: 1.16242 v_acc: 57.9\n",
      "Epoch [3/10], Step[100/352], Loss: 1.0879\n",
      "Epoch [3/10], Step[200/352], Loss: 0.9244\n",
      "Epoch [3/10], Step[300/352], Loss: 1.0153\n",
      "Epoch [3/10], Step[352/352], Loss: 0.9368\n",
      "Epoch[3]: v_loss: 1.13938 v_acc: 59.32\n",
      "Epoch [4/10], Step[100/352], Loss: 1.0604\n",
      "Epoch [4/10], Step[200/352], Loss: 0.8971\n",
      "Epoch [4/10], Step[300/352], Loss: 1.0170\n",
      "Epoch [4/10], Step[352/352], Loss: 0.8986\n",
      "Epoch[4]: v_loss: 1.1464 v_acc: 58.31\n",
      "Epoch [5/10], Step[100/352], Loss: 1.1161\n",
      "Epoch [5/10], Step[200/352], Loss: 0.8958\n",
      "Epoch [5/10], Step[300/352], Loss: 1.0754\n",
      "Epoch [5/10], Step[352/352], Loss: 1.0494\n",
      "Epoch[5]: v_loss: 1.13368 v_acc: 60.2\n",
      "Epoch [6/10], Step[100/352], Loss: 1.1912\n",
      "Epoch [6/10], Step[200/352], Loss: 1.0648\n",
      "Epoch [6/10], Step[300/352], Loss: 0.8949\n",
      "Epoch [6/10], Step[352/352], Loss: 1.1577\n",
      "Epoch[6]: v_loss: 1.13172 v_acc: 59.9\n",
      "Epoch [7/10], Step[100/352], Loss: 1.0635\n",
      "Epoch [7/10], Step[200/352], Loss: 1.1031\n",
      "Epoch [7/10], Step[300/352], Loss: 1.4504\n",
      "Epoch [7/10], Step[352/352], Loss: 0.8274\n",
      "Epoch[7]: v_loss: 1.12255 v_acc: 60.58\n",
      "Epoch [8/10], Step[100/352], Loss: 1.0844\n",
      "Epoch [8/10], Step[200/352], Loss: 1.0778\n",
      "Epoch [8/10], Step[300/352], Loss: 1.1837\n",
      "Epoch [8/10], Step[352/352], Loss: 1.1143\n",
      "Epoch[8]: v_loss: 1.12277 v_acc: 60.5\n",
      "Epoch [9/10], Step[100/352], Loss: 1.0528\n",
      "Epoch [9/10], Step[200/352], Loss: 1.0865\n",
      "Epoch [9/10], Step[300/352], Loss: 1.1371\n",
      "Epoch [9/10], Step[352/352], Loss: 1.0396\n",
      "Epoch[9]: v_loss: 1.09902 v_acc: 60.79\n",
      "Epoch [10/10], Step[100/352], Loss: 1.0044\n",
      "Epoch [10/10], Step[200/352], Loss: 1.0355\n",
      "Epoch [10/10], Step[300/352], Loss: 0.9790\n",
      "Epoch [10/10], Step[352/352], Loss: 1.0428\n",
      "Epoch[10]: v_loss: 1.13671 v_acc: 59.45\n",
      "Best model saved at epoch:  9\n",
      "Best acc model saved at epoch:  9\n",
      "Accuracy of the network on the 10000 test images: 60.79 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "185742.19814912"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define your model (assuming GeneralEEModel is defined elsewhere)\n",
    "# model = GeneralEEModel().to(device)\n",
    "model = Baseline().to(device)\n",
    "model_size, params = print_full_model(model)\n",
    "print(\"Baseline-S Model Size:\", model_size, \"KB\")\n",
    "print(\"Baseline-S Model Params:\", params)\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "dataset = CIFAR10(root='./data', download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root='./data', train=False, transform=ToTensor())\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 128\n",
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, num_workers=4)\n",
    "\n",
    "# Define other parameters\n",
    "target_size = 200  # Target size in KB\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "regularizerParam = 0.0\n",
    "compressionStep = 0.1\n",
    "initialCompressionStep = 0.1\n",
    "fastCompression = False\n",
    "modelName = \"compressed_model\"\n",
    "device = \"cpu\"\n",
    "accuracyAware = True\n",
    "layersFactorization = True\n",
    "calculateInputs = None\n",
    "\n",
    "# Call the compress_NN_models function\n",
    "compress_NN_models(\n",
    "    model, target_size, train_loader, test_loader,\n",
    "    val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate,\n",
    "    criterion=criterion, regularizerParam=regularizerParam, compressionStep=compressionStep,\n",
    "    initialCompressionStep=initialCompressionStep, fastCompression=fastCompression,\n",
    "    modelName=modelName, device=device, accuracyAware=accuracyAware,\n",
    "    layersFactorization=layersFactorization, calculateInputs=calculateInputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline-S Model Size: 718.376 KB\n",
      "Baseline-S Model Params: 179594\n",
      "Directory 'S_baseline_compressed_model_NEW2' created\n",
      "Directory 'S_baseline_compressed_model_NEW2/headers' created\n",
      "Values: 0 0\n",
      "\n",
      "Iterating over the module:\n",
      "\n",
      "Weight shape: torch.Size([64, 3, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 64, 30, 30])\n",
      "Layer conv 1 Values: 1 1\n",
      "Shape output of the module: torch.Size([1, 64, 15, 15])\n",
      "Output Dimension: 4\n",
      "Layer pooling 1 Values: 2 2\n",
      "Weight shape: torch.Size([128, 64, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 128, 13, 13])\n",
      "Layer conv 2 Values: 3 3\n",
      "Shape output of the module: torch.Size([1, 128, 6, 6])\n",
      "Output Dimension: 4\n",
      "Layer pooling 2 Values: 4 4\n",
      "Weight shape: torch.Size([64, 128, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 64, 4, 4])\n",
      "Layer conv 3 Values: 5 5\n",
      "Shape output of the module: torch.Size([1, 64, 2, 2])\n",
      "Output Dimension: 4\n",
      "Layer pooling 3 Values: 6 6\n",
      "Shape output of the module: torch.Size([1, 256])\n",
      "Layer flatten 1 Values: 7 7\n",
      "Weight shape: torch.Size([25, 256])\n",
      "Shape output of the module: torch.Size([1, 25])\n",
      "Layer fc 1 Values: 8 8\n",
      "Weight shape: torch.Size([256, 25])\n",
      "Shape output of the module: torch.Size([1, 256])\n",
      "Layer fc 2 Values: 9 9\n",
      "Weight shape: torch.Size([64, 256])\n",
      "Shape output of the module: torch.Size([1, 64])\n",
      "Layer fc 3 Values: 10 10\n",
      "Weight shape: torch.Size([10, 64])\n",
      "Shape output of the module: torch.Size([1, 10])\n",
      "Layer fc 4 Values: 11 11\n",
      "\n",
      "Checked 11 / 11 layers\n",
      "Saved 11 / 11 layers\n",
      "\n",
      "Ignored these modules:\n",
      "[]\n",
      "\n",
      "\n",
      "Finished Saving the Model\n"
     ]
    }
   ],
   "source": [
    "model = Baseline_compressed().to(device)\n",
    "# print('Expected model keys: \\n',model.state_dict().keys())  # Expected keys\n",
    "# print('loaded model keys: \\n',torch.load(\"compressed_model_186.h5\").keys())  # Loaded keys\n",
    "model.load_state_dict(torch.load(\"/home/mal/DScale/freeml/FreeML/SparseComp/compressed_model_452.h5\", map_location='cpu'))\n",
    "model_size, params = print_full_model(model)\n",
    "print(\"Baseline-S Model Size:\", model_size, \"KB\")\n",
    "print(\"Baseline-S Model Params:\", params)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Download dataset and get a single sample\n",
    "single_sample, _ = dataset[0]  # Extract first sample (image, label)\n",
    "\n",
    "# Add batch dimension\n",
    "single_sample = single_sample.unsqueeze(0)  # Shape: (1, 3, 32, 32)\n",
    "\n",
    "save_compressed_model(model, 'csr', input_data=single_sample, directory='S_baseline_compressed_model_NEW2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_flex_vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
