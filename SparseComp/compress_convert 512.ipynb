{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Compress Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sparsecomp import compress_NN_models\n",
    "import torch\n",
    "from conversion import save_compressed_model\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_model(model):\n",
    "    assert isinstance(model, nn.Module), \"The model is not a subclass of torch.nn.Module\"\n",
    "    kb = 1000\n",
    "    model_size = 0\n",
    "    params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_size = param.nelement() * param.element_size()\n",
    "        model_size += layer_size\n",
    "        # print(name,\"\\t\", param.nelement(), \"\\t\", param.element_size(),\"\\t\", layer_size / kb, \"KB\")\n",
    "\n",
    "    for name, buffer in model.named_buffers():\n",
    "        layer_size = buffer.nelement() * buffer.element_size()\n",
    "        model_size += layer_size\n",
    "        # print(name,\"\\t\", layer_size / kb, \"KB\")\n",
    "    # print(\"Model Size:\", model_size / kb, \"KB\")\n",
    "\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    # print(\"Model Params:\", params)\n",
    "\n",
    "    return (model_size / kb), params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline_compressed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline_compressed, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"valid\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=25, bias=False),\n",
    "            nn.Linear(in_features=25, out_features=256)\n",
    "        )\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        exit_outputs = []\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        exit_outputs.append(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1[0](x)\n",
    "        x = self.fc1[1](x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x, exit_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"valid\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        exit_outputs = []\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        exit_outputs.append(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x, exit_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for S_earlyexit_model\n",
    "class GeneralEEModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneralEEModel, self).__init__()\n",
    "        self.pool_kernels = [\n",
    "            (1, 6, 6), (1, 3, 3), (1, 1, 1)\n",
    "        ]\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc = nn.Linear(in_features=1024, out_features=10)\n",
    "    \n",
    "    def forward(self, x, inference=False):\n",
    "        pooled_outs = []\n",
    "        for layer, out in enumerate(x):\n",
    "            pool_3d = nn.MaxPool3d(kernel_size=self.pool_kernels[layer])\n",
    "            pooled_outs.append(pool_3d(x))\n",
    "        x = torch.cat(pooled_outs, dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        scores = self.fc(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline-S Model Size: 929.32 KB\n",
      "Baseline-S Model Params: 232330\n",
      "Files already downloaded and verified\n",
      "Target size requested: 512 KB\n",
      "Iteration: 0\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 128, 13, 13]) 86528 295424 295424\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv2 with memory usage: 288.500 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 1\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv3 with memory usage: 288.250 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 2\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc1 with memory usage: 257.000 KB\n",
      "Sequential(\n",
      "  (0): Linear(in_features=256, out_features=25, bias=False)\n",
      "  (1): Linear(in_features=25, out_features=256, bias=True)\n",
      ")\n",
      "Iteration: 3\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 25]) 100 25600 25600\n",
      "torch.Size([1, 256]) 1024 26624 26624\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc2 with memory usage: 64.250 KB\n",
      "Layer already small 65792\n",
      "-------------- FINISHED FACTORIZATION --------------\n",
      "Test accuracy: 10.0\n",
      "----------------------------------------------------\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 1728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 16384, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 640, Zero Elements: 0\n",
      "Starting Density of model's parameters: [1, 1, 1, 1, 1, 1, 1]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Layer 1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 1:\t\t 7.168 KB, \tweight:\t 6.912 KB, \tbias: 0.256 KB\n",
      "Layer 2 Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 2:\t\t 295.424 KB, \tweight:\t 294.912 KB, \tbias: 0.512 KB\n",
      "Layer 3 Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 3:\t\t 295.168 KB, \tweight:\t 294.912 KB, \tbias: 0.256 KB\n",
      "Layer 4 Linear(in_features=256, out_features=25, bias=False)\n",
      "Layer 4:\t\t 25.6 KB, \tweight:\t 25.6 KB, \tbias: 0.0 KB\n",
      "Layer 5 Linear(in_features=25, out_features=256, bias=True)\n",
      "Layer 5:\t\t 26.624 KB, \tweight:\t 25.6 KB, \tbias: 1.024 KB\n",
      "Layer 6 Linear(in_features=256, out_features=64, bias=True)\n",
      "Layer 6:\t\t 65.792 KB, \tweight:\t 65.536 KB, \tbias: 0.256 KB\n",
      "Layer 7 Linear(in_features=64, out_features=10, bias=True)\n",
      "Layer 7:\t\t 2.6 KB, \tweight:\t 2.56 KB, \tbias: 0.04 KB\n",
      "Size FC Layer (no sparsity):\t 718.376 KB\n",
      "Size FC Layer (with sparsity):\t 718.376 KB\n",
      "Total Size no sparsity:\t\t 718.376 KB\n",
      "Total Size with sparsity:\t 718.376 KB\n",
      "Total Size with sparsity and CSC representation:\t 718.376 KB\n",
      "-------------------------------------------------------------------------------------------\n",
      "uncomp: 718.376 KB\n",
      "comp: 718.376 KB\n",
      "Starting size of the model: 718.376 KB\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n",
      "Epoch [1/10], Step[100/352], Loss: 2.1057\n",
      "Epoch [1/10], Step[200/352], Loss: 1.7814\n",
      "Epoch [1/10], Step[300/352], Loss: 1.5449\n",
      "Epoch [1/10], Step[352/352], Loss: 1.8293\n",
      "Epoch[1]: v_loss: 1.62987 v_acc: 39.12\n",
      "Epoch [2/10], Step[100/352], Loss: 1.5453\n",
      "Epoch [2/10], Step[200/352], Loss: 1.4729\n",
      "Epoch [2/10], Step[300/352], Loss: 1.4096\n",
      "Epoch [2/10], Step[352/352], Loss: 1.4190\n",
      "Epoch[2]: v_loss: 1.47055 v_acc: 46.07\n",
      "Epoch [3/10], Step[100/352], Loss: 1.4523\n",
      "Epoch [3/10], Step[200/352], Loss: 1.2702\n",
      "Epoch [3/10], Step[300/352], Loss: 1.4199\n",
      "Epoch [3/10], Step[352/352], Loss: 1.4628\n",
      "Epoch[3]: v_loss: 1.39603 v_acc: 48.37\n",
      "Epoch [4/10], Step[100/352], Loss: 1.2698\n",
      "Epoch [4/10], Step[200/352], Loss: 1.3063\n",
      "Epoch [4/10], Step[300/352], Loss: 1.3868\n",
      "Epoch [4/10], Step[352/352], Loss: 1.1288\n",
      "Epoch[4]: v_loss: 1.27925 v_acc: 53.55\n",
      "Epoch [5/10], Step[100/352], Loss: 1.2424\n",
      "Epoch [5/10], Step[200/352], Loss: 1.2295\n",
      "Epoch [5/10], Step[300/352], Loss: 1.2966\n",
      "Epoch [5/10], Step[352/352], Loss: 1.0988\n",
      "Epoch[5]: v_loss: 1.25014 v_acc: 54.63\n",
      "Epoch [6/10], Step[100/352], Loss: 1.1201\n",
      "Epoch [6/10], Step[200/352], Loss: 1.1308\n",
      "Epoch [6/10], Step[300/352], Loss: 1.1306\n",
      "Epoch [6/10], Step[352/352], Loss: 1.1438\n",
      "Epoch[6]: v_loss: 1.24776 v_acc: 55.57\n",
      "Epoch [7/10], Step[100/352], Loss: 0.9514\n",
      "Epoch [7/10], Step[200/352], Loss: 1.2317\n",
      "Epoch [7/10], Step[300/352], Loss: 1.1089\n",
      "Epoch [7/10], Step[352/352], Loss: 1.0860\n",
      "Epoch[7]: v_loss: 1.1795 v_acc: 57.62\n",
      "Epoch [8/10], Step[100/352], Loss: 1.0843\n",
      "Epoch [8/10], Step[200/352], Loss: 1.0464\n",
      "Epoch [8/10], Step[300/352], Loss: 1.2316\n",
      "Epoch [8/10], Step[352/352], Loss: 1.1477\n",
      "Epoch[8]: v_loss: 1.21425 v_acc: 56.67\n",
      "Epoch [9/10], Step[100/352], Loss: 1.0062\n",
      "Epoch [9/10], Step[200/352], Loss: 1.1618\n",
      "Epoch [9/10], Step[300/352], Loss: 1.2091\n",
      "Epoch [9/10], Step[352/352], Loss: 0.9815\n",
      "Epoch[9]: v_loss: 1.15563 v_acc: 58.65\n",
      "Epoch [10/10], Step[100/352], Loss: 1.0637\n",
      "Epoch [10/10], Step[200/352], Loss: 1.1697\n",
      "Epoch [10/10], Step[300/352], Loss: 1.0991\n",
      "Epoch [10/10], Step[352/352], Loss: 1.0287\n",
      "Epoch[10]: v_loss: 1.14554 v_acc: 59.76\n",
      "Best model saved at epoch:  10\n",
      "Best acc model saved at epoch:  10\n",
      "Accuracy of the network on the 10000 test images: 59.76 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "452187.09907456"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define your model (assuming GeneralEEModel is defined elsewhere)\n",
    "# model = GeneralEEModel().to(device)\n",
    "model = Baseline().to(device)\n",
    "model_size, params = print_full_model(model)\n",
    "print(\"Baseline-S Model Size:\", model_size, \"KB\")\n",
    "print(\"Baseline-S Model Params:\", params)\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "dataset = CIFAR10(root='./data', download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root='./data', train=False, transform=ToTensor())\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 128\n",
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, num_workers=4)\n",
    "\n",
    "# Define other parameters\n",
    "target_size = 512  # Target size in KB\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "regularizerParam = 0.0\n",
    "compressionStep = 0.1\n",
    "initialCompressionStep = 0.1\n",
    "fastCompression = False\n",
    "modelName = \"_512KB\"\n",
    "device = \"cpu\"\n",
    "accuracyAware = True\n",
    "layersFactorization = True\n",
    "calculateInputs = None\n",
    "\n",
    "# Call the compress_NN_models function\n",
    "compress_NN_models(\n",
    "    model, target_size, train_loader, test_loader,\n",
    "    val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate,\n",
    "    criterion=criterion, regularizerParam=regularizerParam, compressionStep=compressionStep,\n",
    "    initialCompressionStep=initialCompressionStep, fastCompression=fastCompression,\n",
    "    modelName=modelName, device=device, accuracyAware=accuracyAware,\n",
    "    layersFactorization=layersFactorization, calculateInputs=calculateInputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Target size requested: 200 KB\n",
      "Iteration: 0\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 128, 13, 13]) 86528 295424 295424\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv2 with memory usage: 288.500 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 1\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv3 with memory usage: 288.250 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 2\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc1 with memory usage: 257.000 KB\n",
      "Sequential(\n",
      "  (0): Linear(in_features=256, out_features=25, bias=False)\n",
      "  (1): Linear(in_features=25, out_features=256, bias=True)\n",
      ")\n",
      "Iteration: 3\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 25]) 100 25600 25600\n",
      "torch.Size([1, 256]) 1024 26624 26624\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc2 with memory usage: 64.250 KB\n",
      "Layer already small 65792\n",
      "-------------- FINISHED FACTORIZATION --------------\n",
      "Test accuracy: 10.0\n",
      "----------------------------------------------------\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 1728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 16384, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 640, Zero Elements: 0\n",
      "Starting Density of model's parameters: [1, 1, 1, 1, 1, 1, 1]\n",
      "Starting size of the model: 718.376 KB\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n",
      "Epoch [1/10], Step[100/352], Loss: 2.0556\n",
      "Epoch [1/10], Step[200/352], Loss: 1.8050\n",
      "Epoch [1/10], Step[300/352], Loss: 1.7328\n",
      "Epoch [1/10], Step[352/352], Loss: 1.8614\n",
      "Epoch[1]: v_loss: 1.63954 v_acc: 38.8\n",
      "Epoch [2/10], Step[100/352], Loss: 1.5803\n",
      "Epoch [2/10], Step[200/352], Loss: 1.4922\n",
      "Epoch [2/10], Step[300/352], Loss: 1.5496\n",
      "Epoch [2/10], Step[352/352], Loss: 1.4441\n",
      "Epoch[2]: v_loss: 1.49792 v_acc: 44.0\n",
      "Epoch [3/10], Step[100/352], Loss: 1.4393\n",
      "Epoch [3/10], Step[200/352], Loss: 1.4913\n",
      "Epoch [3/10], Step[300/352], Loss: 1.3059\n",
      "Epoch [3/10], Step[352/352], Loss: 1.5887\n",
      "Epoch[3]: v_loss: 1.39102 v_acc: 48.43\n",
      "Epoch [4/10], Step[100/352], Loss: 1.4242\n",
      "Epoch [4/10], Step[200/352], Loss: 1.2871\n",
      "Epoch [4/10], Step[300/352], Loss: 1.3223\n",
      "Epoch [4/10], Step[352/352], Loss: 1.1258\n",
      "Epoch[4]: v_loss: 1.34912 v_acc: 50.44\n",
      "Epoch [5/10], Step[100/352], Loss: 1.4471\n",
      "Epoch [5/10], Step[200/352], Loss: 1.2930\n",
      "Epoch [5/10], Step[300/352], Loss: 1.3145\n",
      "Epoch [5/10], Step[352/352], Loss: 1.2659\n",
      "Epoch[5]: v_loss: 1.2565 v_acc: 53.84\n",
      "Epoch [6/10], Step[100/352], Loss: 0.9845\n",
      "Epoch [6/10], Step[200/352], Loss: 1.1710\n",
      "Epoch [6/10], Step[300/352], Loss: 1.1355\n",
      "Epoch [6/10], Step[352/352], Loss: 1.2812\n",
      "Epoch[6]: v_loss: 1.24036 v_acc: 54.67\n",
      "Epoch [7/10], Step[100/352], Loss: 1.1293\n",
      "Epoch [7/10], Step[200/352], Loss: 1.2322\n",
      "Epoch [7/10], Step[300/352], Loss: 1.0568\n",
      "Epoch [7/10], Step[352/352], Loss: 1.1905\n",
      "Epoch[7]: v_loss: 1.18402 v_acc: 57.43\n",
      "Epoch [8/10], Step[100/352], Loss: 1.2256\n",
      "Epoch [8/10], Step[200/352], Loss: 1.1091\n",
      "Epoch [8/10], Step[300/352], Loss: 1.1191\n",
      "Epoch [8/10], Step[352/352], Loss: 1.2661\n",
      "Epoch[8]: v_loss: 1.20069 v_acc: 56.58\n",
      "Epoch [9/10], Step[100/352], Loss: 1.1244\n",
      "Epoch [9/10], Step[200/352], Loss: 1.2998\n",
      "Epoch [9/10], Step[300/352], Loss: 1.1434\n",
      "Epoch [9/10], Step[352/352], Loss: 1.2101\n",
      "Epoch[9]: v_loss: 1.14524 v_acc: 59.34\n",
      "Epoch [10/10], Step[100/352], Loss: 1.0616\n",
      "Epoch [10/10], Step[200/352], Loss: 1.2180\n",
      "Epoch [10/10], Step[300/352], Loss: 1.2693\n",
      "Epoch [10/10], Step[352/352], Loss: 0.9335\n",
      "Epoch[10]: v_loss: 1.16423 v_acc: 57.94\n",
      "Best model saved at epoch:  9\n",
      "Best acc model saved at epoch:  9\n",
      "Accuracy of the network on the 10000 test images: 59.34 %\n",
      "0 iteration -  Size: 452187.09907456 [1, 0.04782969, 1, 1, 1, 1, 1]\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n",
      "Epoch [1/10], Step[100/352], Loss: 1.2743\n",
      "Epoch [1/10], Step[200/352], Loss: 1.1753\n",
      "Epoch [1/10], Step[300/352], Loss: 1.0715\n",
      "Epoch [1/10], Step[352/352], Loss: 1.2733\n",
      "Epoch[1]: v_loss: 1.19059 v_acc: 57.34\n",
      "Epoch [2/10], Step[100/352], Loss: 1.0868\n",
      "Epoch [2/10], Step[200/352], Loss: 1.0783\n",
      "Epoch [2/10], Step[300/352], Loss: 1.0607\n",
      "Epoch [2/10], Step[352/352], Loss: 0.9009\n",
      "Epoch[2]: v_loss: 1.16242 v_acc: 57.9\n",
      "Epoch [3/10], Step[100/352], Loss: 1.0879\n",
      "Epoch [3/10], Step[200/352], Loss: 0.9244\n",
      "Epoch [3/10], Step[300/352], Loss: 1.0153\n",
      "Epoch [3/10], Step[352/352], Loss: 0.9368\n",
      "Epoch[3]: v_loss: 1.13938 v_acc: 59.32\n",
      "Epoch [4/10], Step[100/352], Loss: 1.0604\n",
      "Epoch [4/10], Step[200/352], Loss: 0.8971\n",
      "Epoch [4/10], Step[300/352], Loss: 1.0170\n",
      "Epoch [4/10], Step[352/352], Loss: 0.8986\n",
      "Epoch[4]: v_loss: 1.1464 v_acc: 58.31\n",
      "Epoch [5/10], Step[100/352], Loss: 1.1161\n",
      "Epoch [5/10], Step[200/352], Loss: 0.8958\n",
      "Epoch [5/10], Step[300/352], Loss: 1.0754\n",
      "Epoch [5/10], Step[352/352], Loss: 1.0494\n",
      "Epoch[5]: v_loss: 1.13368 v_acc: 60.2\n",
      "Epoch [6/10], Step[100/352], Loss: 1.1912\n",
      "Epoch [6/10], Step[200/352], Loss: 1.0648\n",
      "Epoch [6/10], Step[300/352], Loss: 0.8949\n",
      "Epoch [6/10], Step[352/352], Loss: 1.1577\n",
      "Epoch[6]: v_loss: 1.13172 v_acc: 59.9\n",
      "Epoch [7/10], Step[100/352], Loss: 1.0635\n",
      "Epoch [7/10], Step[200/352], Loss: 1.1031\n",
      "Epoch [7/10], Step[300/352], Loss: 1.4504\n",
      "Epoch [7/10], Step[352/352], Loss: 0.8274\n",
      "Epoch[7]: v_loss: 1.12255 v_acc: 60.58\n",
      "Epoch [8/10], Step[100/352], Loss: 1.0844\n",
      "Epoch [8/10], Step[200/352], Loss: 1.0778\n",
      "Epoch [8/10], Step[300/352], Loss: 1.1837\n",
      "Epoch [8/10], Step[352/352], Loss: 1.1143\n",
      "Epoch[8]: v_loss: 1.12277 v_acc: 60.5\n",
      "Epoch [9/10], Step[100/352], Loss: 1.0528\n",
      "Epoch [9/10], Step[200/352], Loss: 1.0865\n",
      "Epoch [9/10], Step[300/352], Loss: 1.1371\n",
      "Epoch [9/10], Step[352/352], Loss: 1.0396\n",
      "Epoch[9]: v_loss: 1.09902 v_acc: 60.79\n",
      "Epoch [10/10], Step[100/352], Loss: 1.0044\n",
      "Epoch [10/10], Step[200/352], Loss: 1.0355\n",
      "Epoch [10/10], Step[300/352], Loss: 0.9790\n",
      "Epoch [10/10], Step[352/352], Loss: 1.0428\n",
      "Epoch[10]: v_loss: 1.13671 v_acc: 59.45\n",
      "Best model saved at epoch:  9\n",
      "Best acc model saved at epoch:  9\n",
      "Accuracy of the network on the 10000 test images: 60.79 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "185742.19814912"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define your model (assuming GeneralEEModel is defined elsewhere)\n",
    "# model = GeneralEEModel().to(device)\n",
    "model = Baseline().to(device)\n",
    "model_size, params = print_full_model(model)\n",
    "print(\"Baseline-S Model Size:\", model_size, \"KB\")\n",
    "print(\"Baseline-S Model Params:\", params)\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "dataset = CIFAR10(root='./data', download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root='./data', train=False, transform=ToTensor())\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 128\n",
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, num_workers=4)\n",
    "\n",
    "# Define other parameters\n",
    "target_size = 200  # Target size in KB\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "regularizerParam = 0.0\n",
    "compressionStep = 0.1\n",
    "initialCompressionStep = 0.1\n",
    "fastCompression = False\n",
    "modelName = \"compressed_model\"\n",
    "device = \"cpu\"\n",
    "accuracyAware = True\n",
    "layersFactorization = True\n",
    "calculateInputs = None\n",
    "\n",
    "# Call the compress_NN_models function\n",
    "compress_NN_models(\n",
    "    model, target_size, train_loader, test_loader,\n",
    "    val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate,\n",
    "    criterion=criterion, regularizerParam=regularizerParam, compressionStep=compressionStep,\n",
    "    initialCompressionStep=initialCompressionStep, fastCompression=fastCompression,\n",
    "    modelName=modelName, device=device, accuracyAware=accuracyAware,\n",
    "    layersFactorization=layersFactorization, calculateInputs=calculateInputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline-S Model Size: 718.376 KB\n",
      "Baseline-S Model Params: 179594\n",
      "Directory '_512KB_C_code' created\n",
      "Directory '_512KB_C_code/headers' created\n",
      "Values: 0 0\n",
      "\n",
      "Iterating over the module:\n",
      "\n",
      "Weight shape: torch.Size([64, 3, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 64, 30, 30])\n",
      "Layer conv 1 Values: 1 1\n",
      "Shape output of the module: torch.Size([1, 64, 15, 15])\n",
      "Output Dimension: 4\n",
      "Layer pooling 1 Values: 2 2\n",
      "Weight shape: torch.Size([128, 64, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 128, 13, 13])\n",
      "Layer conv 2 Values: 3 3\n",
      "Shape output of the module: torch.Size([1, 128, 6, 6])\n",
      "Output Dimension: 4\n",
      "Layer pooling 2 Values: 4 4\n",
      "Weight shape: torch.Size([64, 128, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 64, 4, 4])\n",
      "Layer conv 3 Values: 5 5\n",
      "Shape output of the module: torch.Size([1, 64, 2, 2])\n",
      "Output Dimension: 4\n",
      "Layer pooling 3 Values: 6 6\n",
      "Shape output of the module: torch.Size([1, 256])\n",
      "Layer flatten 1 Values: 7 7\n",
      "Weight shape: torch.Size([25, 256])\n",
      "Shape output of the module: torch.Size([1, 25])\n",
      "Layer fc 1 Values: 8 8\n",
      "Weight shape: torch.Size([256, 25])\n",
      "Shape output of the module: torch.Size([1, 256])\n",
      "Layer fc 2 Values: 9 9\n",
      "Weight shape: torch.Size([64, 256])\n",
      "Shape output of the module: torch.Size([1, 64])\n",
      "Layer fc 3 Values: 10 10\n",
      "Weight shape: torch.Size([10, 64])\n",
      "Shape output of the module: torch.Size([1, 10])\n",
      "Layer fc 4 Values: 11 11\n",
      "\n",
      "Checked 11 / 11 layers\n",
      "Saved 11 / 11 layers\n",
      "\n",
      "Ignored these modules:\n",
      "[]\n",
      "\n",
      "\n",
      "Finished Saving the Model\n"
     ]
    }
   ],
   "source": [
    "model = Baseline_compressed().to(device)\n",
    "# print('Expected model keys: \\n',model.state_dict().keys())  # Expected keys\n",
    "# print('loaded model keys: \\n',torch.load(\"compressed_model_186.h5\").keys())  # Loaded keys\n",
    "model.load_state_dict(torch.load(\"/home/mal/DScale/momin_flex_nns/freeml/FreeML/SparseComp/_512KB_452.h5\", map_location='cpu'))\n",
    "model_size, params = print_full_model(model)\n",
    "print(\"Baseline-S Model Size:\", model_size, \"KB\")\n",
    "print(\"Baseline-S Model Params:\", params)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Download dataset and get a single sample\n",
    "single_sample, _ = dataset[0]  # Extract first sample (image, label)\n",
    "\n",
    "# Add batch dimension\n",
    "single_sample = single_sample.unsqueeze(0)  # Shape: (1, 3, 32, 32)\n",
    "\n",
    "save_compressed_model(model, 'csr', input_data=single_sample, directory='_512KB_C_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Baseline_compressed().to(device)\n",
    "# print('Expected model keys: \\n',model.state_dict().keys())  # Expected keys\n",
    "# print('loaded model keys: \\n',torch.load(\"compressed_model_186.h5\").keys())  # Loaded keys\n",
    "model.load_state_dict(torch.load(\"/home/mal/DScale/momin_flex_nns/freeml/FreeML/SparseComp/models/512KB/_512KB_452.h5\", map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 30, 30]           1,792\n",
      "         MaxPool2d-2           [-1, 64, 15, 15]               0\n",
      "            Conv2d-3          [-1, 128, 13, 13]          73,856\n",
      "         MaxPool2d-4            [-1, 128, 6, 6]               0\n",
      "            Conv2d-5             [-1, 64, 4, 4]          73,792\n",
      "         MaxPool2d-6             [-1, 64, 2, 2]               0\n",
      "           Flatten-7                  [-1, 256]               0\n",
      "            Linear-8                   [-1, 25]           6,400\n",
      "            Linear-9                  [-1, 256]           6,656\n",
      "           Linear-10                   [-1, 64]          16,448\n",
      "           Linear-11                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 179,594\n",
      "Trainable params: 179,594\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.76\n",
      "Params size (MB): 0.69\n",
      "Estimated Total Size (MB): 1.46\n",
      "----------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "Actual active weights per layer\n",
      "\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "conv1 1728\n",
      "conv2 3527\n",
      "conv3 73728\n",
      "fc1.0 6400\n",
      "fc1.1 6400\n",
      "fc2 16384\n",
      "fc3 640\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3, 32, 32))\n",
    "print(100*'-','\\n')\n",
    "print(\"Actual active weights per layer\\n\")\n",
    "print(100*'-','\\n')\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, (torch.nn.Conv2d, torch.nn.Linear)):\n",
    "        print(name, layer.weight.data.nonzero().size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::_convolution_mode encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([1, 3, 32, 32])\n",
      "Layer: conv1\n",
      "    Dense FLOPs: 0.00 MFLOPs\n",
      "    Active FLOPs (considering non-zero weights): 0.00 MFLOPs\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 64, 3, 3], expected input[1, 3, 32, 32] to have 64 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tensor shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Calculate FLOPs with active weights\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43mcalculate_flops_with_active_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 23\u001b[0m, in \u001b[0;36mcalculate_flops_with_active_weights\u001b[0;34m(model, input_tensor)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, (torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv2d, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear)):  \u001b[38;5;66;03m# Only consider layers with weights\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Compute FLOPs for the dense layer (no sparsity considered yet)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     flops \u001b[38;5;241m=\u001b[39m FlopCountAnalysis(layer, input_tensor)\n\u001b[0;32m---> 23\u001b[0m     dense_flops \u001b[38;5;241m=\u001b[39m \u001b[43mflops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Total FLOPs for the layer (dense)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Get the count of non-zero weights (active weights)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     nnz \u001b[38;5;241m=\u001b[39m get_non_zero_weights_count(layer)\n",
      "File \u001b[0;32m/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/fvcore/nn/jit_analysis.py:248\u001b[0m, in \u001b[0;36mJitModelAnalysis.total\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    Returns the total aggregated statistic across all operators\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    for the requested module.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m        int : The aggregated statistic.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     module_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical_module_name(module_name)\n\u001b[1;32m    250\u001b[0m     total_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(stats\u001b[38;5;241m.\u001b[39mcounts[module_name]\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/fvcore/nn/jit_analysis.py:551\u001b[0m, in \u001b[0;36mJitModelAnalysis._analyze\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_trace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_tracer_warning\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    550\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39mTracerWarning)\n\u001b[0;32m--> 551\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_get_scoped_trace_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aliases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# Assures even modules not in the trace graph are initialized to zero count\u001b[39;00m\n\u001b[1;32m    554\u001b[0m counts \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/fvcore/nn/jit_analysis.py:176\u001b[0m, in \u001b[0;36m_get_scoped_trace_graph\u001b[0;34m(module, inputs, aliases)\u001b[0m\n\u001b[1;32m    173\u001b[0m     name \u001b[38;5;241m=\u001b[39m aliases[mod]\n\u001b[1;32m    174\u001b[0m     register_hooks(mod, name)\n\u001b[0;32m--> 176\u001b[0m graph, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m hook_handles:\n\u001b[1;32m    179\u001b[0m     handle\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[0;32m/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/torch/jit/_trace.py:1184\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1183\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m-> 1184\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mONNXTracedModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/torch/jit/_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 127\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/torch/jit/_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    117\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 118\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    120\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/torch/nn/modules/module.py:1182\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 64, 3, 3], expected input[1, 3, 32, 32] to have 64 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "# Helper function to get non-zero weights count for each layer\n",
    "def get_non_zero_weights_count(layer):\n",
    "    # Get layer weights (e.g., convolution weights or linear layer weights)\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        return layer.weight.data.nonzero().size(0)\n",
    "    elif isinstance(layer, torch.nn.Linear):\n",
    "        return layer.weight.data.nonzero().size(0)\n",
    "    return 0  # For layers that don't have weights (like pooling)\n",
    "\n",
    "# Function to calculate FLOPs for each layer considering sparse weights\n",
    "def calculate_flops_with_active_weights(model, input_tensor):\n",
    "    total_flops = 0  # Keep track of the total FLOPs\n",
    "    total_params = 0  # Keep track of total params (for adjusting based on non-zero weights)\n",
    "    \n",
    "    # Loop through each layer in the model\n",
    "    for name, layer in model.named_modules():\n",
    "        if isinstance(layer, (torch.nn.Conv2d, torch.nn.Linear)):  # Only consider layers with weights\n",
    "            # Compute FLOPs for the dense layer (no sparsity considered yet)\n",
    "            flops = FlopCountAnalysis(layer, input_tensor)\n",
    "            dense_flops = flops.total()  # Total FLOPs for the layer (dense)\n",
    "\n",
    "            # Get the count of non-zero weights (active weights)\n",
    "            nnz = get_non_zero_weights_count(layer)\n",
    "            layer_params = layer.weight.numel()  # Total number of parameters in the layer\n",
    "            \n",
    "            # Adjust FLOPs based on active weights (nnz / total_params)\n",
    "            active_weight_fraction = nnz / layer_params  # Fraction of non-zero weights\n",
    "            active_flops = dense_flops * active_weight_fraction  # Adjusted FLOPs for sparse model\n",
    "            \n",
    "            print(f\"Layer: {name}\")\n",
    "            print(f\"    Dense FLOPs: {dense_flops / 1e6:.2f} MFLOPs\")\n",
    "            print(f\"    Active FLOPs (considering non-zero weights): {active_flops / 1e6:.2f} MFLOPs\")\n",
    "            \n",
    "            total_flops += active_flops\n",
    "            total_params += layer_params\n",
    "    \n",
    "    print(f\"Total Active FLOPs (Model): {total_flops / 1e6:.2f} MFLOPs\")\n",
    "    print(f\"Total Params (Model): {total_params}\")\n",
    "\n",
    "# Example input tensor size (e.g., for an image with 3 channels of size 64x64)\n",
    "input_tensor = torch.randn(128, 64, 32, 32).to(device)\n",
    "print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "\n",
    "# Calculate FLOPs with active weights\n",
    "calculate_flops_with_active_weights(model, input_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_flex_vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
