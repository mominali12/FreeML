{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Compress Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sparsecomp import compress_NN_models\n",
    "import torch\n",
    "from conversion import save_compressed_model\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"GPU-2921ac16-4f98-2a38-7872-ac2d9fa97e35\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_model(model):\n",
    "    assert isinstance(model, nn.Module), \"The model is not a subclass of torch.nn.Module\"\n",
    "    kb = 1000\n",
    "    model_size = 0\n",
    "    params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_size = param.nelement() * param.element_size()\n",
    "        model_size += layer_size\n",
    "        # print(name,\"\\t\", param.nelement(), \"\\t\", param.element_size(),\"\\t\", layer_size / kb, \"KB\")\n",
    "\n",
    "    for name, buffer in model.named_buffers():\n",
    "        layer_size = buffer.nelement() * buffer.element_size()\n",
    "        model_size += layer_size\n",
    "        # print(name,\"\\t\", layer_size / kb, \"KB\")\n",
    "    # print(\"Model Size:\", model_size / kb, \"KB\")\n",
    "\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    # print(\"Model Params:\", params)\n",
    "\n",
    "    return (model_size / kb), params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline_compressed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline_compressed, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"valid\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=25, bias=False),\n",
    "            nn.Linear(in_features=25, out_features=256)\n",
    "        )\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        exit_outputs = []\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        exit_outputs.append(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1[0](x)\n",
    "        x = self.fc1[1](x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x, exit_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"valid\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        exit_outputs = []\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        exit_outputs.append(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x, exit_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for S_earlyexit_model\n",
    "class GeneralEEModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneralEEModel, self).__init__()\n",
    "        self.pool_kernels = [\n",
    "            (1, 6, 6), (1, 3, 3), (1, 1, 1)\n",
    "        ]\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc = nn.Linear(in_features=1024, out_features=10)\n",
    "    \n",
    "    def forward(self, x, inference=False):\n",
    "        pooled_outs = []\n",
    "        for layer, out in enumerate(x):\n",
    "            pool_3d = nn.MaxPool3d(kernel_size=self.pool_kernels[layer])\n",
    "            pooled_outs.append(pool_3d(x))\n",
    "        x = torch.cat(pooled_outs, dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        scores = self.fc(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline-S Model Size: 929.32 KB\n",
      "Baseline-S Model Params: 232330\n",
      "Files already downloaded and verified\n",
      "Target size requested: 256 KB\n",
      "Iteration: 0\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 128, 13, 13]) 86528 295424 295424\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv2 with memory usage: 288.500 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 1\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv3 with memory usage: 288.250 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 2\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc1 with memory usage: 257.000 KB\n",
      "Sequential(\n",
      "  (0): Linear(in_features=256, out_features=25, bias=False)\n",
      "  (1): Linear(in_features=25, out_features=256, bias=True)\n",
      ")\n",
      "Iteration: 3\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 25]) 100 25600 25600\n",
      "torch.Size([1, 256]) 1024 26624 26624\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc2 with memory usage: 64.250 KB\n",
      "Layer already small 65792\n",
      "-------------- FINISHED FACTORIZATION --------------\n",
      "Test accuracy: 9.96\n",
      "----------------------------------------------------\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 1728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 16384, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 640, Zero Elements: 0\n",
      "Starting Density of model's parameters: [1, 1, 1, 1, 1, 1, 1]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Layer 1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 1:\t\t 7.168 KB, \tweight:\t 6.912 KB, \tbias: 0.256 KB\n",
      "Layer 2 Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 2:\t\t 295.424 KB, \tweight:\t 294.912 KB, \tbias: 0.512 KB\n",
      "Layer 3 Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 3:\t\t 295.168 KB, \tweight:\t 294.912 KB, \tbias: 0.256 KB\n",
      "Layer 4 Linear(in_features=256, out_features=25, bias=False)\n",
      "Layer 4:\t\t 25.6 KB, \tweight:\t 25.6 KB, \tbias: 0.0 KB\n",
      "Layer 5 Linear(in_features=25, out_features=256, bias=True)\n",
      "Layer 5:\t\t 26.624 KB, \tweight:\t 25.6 KB, \tbias: 1.024 KB\n",
      "Layer 6 Linear(in_features=256, out_features=64, bias=True)\n",
      "Layer 6:\t\t 65.792 KB, \tweight:\t 65.536 KB, \tbias: 0.256 KB\n",
      "Layer 7 Linear(in_features=64, out_features=10, bias=True)\n",
      "Layer 7:\t\t 2.6 KB, \tweight:\t 2.56 KB, \tbias: 0.04 KB\n",
      "Size FC Layer (no sparsity):\t 718.376 KB\n",
      "Size FC Layer (with sparsity):\t 718.376 KB\n",
      "Total Size no sparsity:\t\t 718.376 KB\n",
      "Total Size with sparsity:\t 718.376 KB\n",
      "Total Size with sparsity and CSC representation:\t 718.376 KB\n",
      "-------------------------------------------------------------------------------------------\n",
      "uncomp: 718.376 KB\n",
      "comp: 718.376 KB\n",
      "Starting size of the model: 718.376 KB\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n",
      "Epoch [1/10], Step[100/352], Loss: 1.9611\n",
      "Epoch [1/10], Step[200/352], Loss: 1.8740\n",
      "Epoch [1/10], Step[300/352], Loss: 1.6590\n",
      "Epoch [1/10], Step[352/352], Loss: 1.5482\n",
      "Epoch[1]: v_loss: 1.67507 v_acc: 37.64\n",
      "Epoch [2/10], Step[100/352], Loss: 1.7500\n",
      "Epoch [2/10], Step[200/352], Loss: 1.5724\n",
      "Epoch [2/10], Step[300/352], Loss: 1.3325\n",
      "Epoch [2/10], Step[352/352], Loss: 1.4764\n",
      "Epoch[2]: v_loss: 1.49293 v_acc: 44.31\n",
      "Epoch [3/10], Step[100/352], Loss: 1.3314\n",
      "Epoch [3/10], Step[200/352], Loss: 1.4894\n",
      "Epoch [3/10], Step[300/352], Loss: 1.4362\n",
      "Epoch [3/10], Step[352/352], Loss: 1.3107\n",
      "Epoch[3]: v_loss: 1.43479 v_acc: 47.93\n",
      "Epoch [4/10], Step[100/352], Loss: 1.3337\n",
      "Epoch [4/10], Step[200/352], Loss: 1.3864\n",
      "Epoch [4/10], Step[300/352], Loss: 1.3167\n",
      "Epoch [4/10], Step[352/352], Loss: 1.3510\n",
      "Epoch[4]: v_loss: 1.32207 v_acc: 51.57\n",
      "Epoch [5/10], Step[100/352], Loss: 1.2727\n",
      "Epoch [5/10], Step[200/352], Loss: 1.2624\n",
      "Epoch [5/10], Step[300/352], Loss: 1.2132\n",
      "Epoch [5/10], Step[352/352], Loss: 1.4677\n",
      "Epoch[5]: v_loss: 1.31093 v_acc: 52.13\n",
      "Epoch [6/10], Step[100/352], Loss: 1.2064\n",
      "Epoch [6/10], Step[200/352], Loss: 1.3854\n",
      "Epoch [6/10], Step[300/352], Loss: 1.4665\n",
      "Epoch [6/10], Step[352/352], Loss: 1.4506\n",
      "Epoch[6]: v_loss: 1.30071 v_acc: 53.47\n",
      "Epoch [7/10], Step[100/352], Loss: 1.1165\n",
      "Epoch [7/10], Step[200/352], Loss: 1.2675\n",
      "Epoch [7/10], Step[300/352], Loss: 1.1508\n",
      "Epoch [7/10], Step[352/352], Loss: 1.3899\n",
      "Epoch[7]: v_loss: 1.25313 v_acc: 55.57\n",
      "Epoch [8/10], Step[100/352], Loss: 1.2902\n",
      "Epoch [8/10], Step[200/352], Loss: 1.0548\n",
      "Epoch [8/10], Step[300/352], Loss: 0.9912\n",
      "Epoch [8/10], Step[352/352], Loss: 1.1102\n",
      "Epoch[8]: v_loss: 1.2889 v_acc: 55.38\n",
      "Epoch [9/10], Step[100/352], Loss: 1.1803\n",
      "Epoch [9/10], Step[200/352], Loss: 1.1841\n",
      "Epoch [9/10], Step[300/352], Loss: 1.1896\n",
      "Epoch [9/10], Step[352/352], Loss: 1.4085\n",
      "Epoch[9]: v_loss: 1.23828 v_acc: 55.68\n",
      "Epoch [10/10], Step[100/352], Loss: 1.0461\n",
      "Epoch [10/10], Step[200/352], Loss: 1.0690\n",
      "Epoch [10/10], Step[300/352], Loss: 1.1017\n",
      "Epoch [10/10], Step[352/352], Loss: 1.0595\n",
      "Epoch[10]: v_loss: 1.16191 v_acc: 59.86\n",
      "Best model saved at epoch:  10\n",
      "Best acc model saved at epoch:  10\n",
      "Accuracy of the network on the 10000 test images: 59.86 %\n",
      "0 iteration -  Size: 452187.09907456 [1, 0.04782969, 1, 1, 1, 1, 1]\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n",
      "Epoch [1/10], Step[100/352], Loss: 1.1560\n",
      "Epoch [1/10], Step[200/352], Loss: 1.1388\n",
      "Epoch [1/10], Step[300/352], Loss: 1.1535\n",
      "Epoch [1/10], Step[352/352], Loss: 1.3657\n",
      "Epoch[1]: v_loss: 1.16045 v_acc: 59.27\n",
      "Epoch [2/10], Step[100/352], Loss: 1.0892\n",
      "Epoch [2/10], Step[200/352], Loss: 1.2166\n",
      "Epoch [2/10], Step[300/352], Loss: 1.0648\n",
      "Epoch [2/10], Step[352/352], Loss: 0.9586\n",
      "Epoch[2]: v_loss: 1.16471 v_acc: 59.34\n",
      "Epoch [3/10], Step[100/352], Loss: 0.9739\n",
      "Epoch [3/10], Step[200/352], Loss: 1.1357\n",
      "Epoch [3/10], Step[300/352], Loss: 1.0402\n",
      "Epoch [3/10], Step[352/352], Loss: 0.9823\n",
      "Epoch[3]: v_loss: 1.14402 v_acc: 59.7\n",
      "Epoch [4/10], Step[100/352], Loss: 1.3546\n",
      "Epoch [4/10], Step[200/352], Loss: 1.0114\n",
      "Epoch [4/10], Step[300/352], Loss: 1.2212\n",
      "Epoch [4/10], Step[352/352], Loss: 1.2359\n",
      "Epoch[4]: v_loss: 1.16058 v_acc: 59.38\n",
      "Epoch [5/10], Step[100/352], Loss: 0.9642\n",
      "Epoch [5/10], Step[200/352], Loss: 1.1796\n",
      "Epoch [5/10], Step[300/352], Loss: 1.1513\n",
      "Epoch [5/10], Step[352/352], Loss: 1.0892\n",
      "Epoch[5]: v_loss: 1.17147 v_acc: 59.34\n",
      "Epoch [6/10], Step[100/352], Loss: 1.1275\n",
      "Epoch [6/10], Step[200/352], Loss: 1.0915\n",
      "Epoch [6/10], Step[300/352], Loss: 1.0721\n",
      "Epoch [6/10], Step[352/352], Loss: 1.1034\n",
      "Epoch[6]: v_loss: 1.15163 v_acc: 60.07\n",
      "Epoch [7/10], Step[100/352], Loss: 1.0533\n",
      "Epoch [7/10], Step[200/352], Loss: 1.0557\n",
      "Epoch [7/10], Step[300/352], Loss: 1.0367\n",
      "Epoch [7/10], Step[352/352], Loss: 0.9786\n",
      "Epoch[7]: v_loss: 1.12617 v_acc: 60.19\n",
      "Epoch [8/10], Step[100/352], Loss: 0.9170\n",
      "Epoch [8/10], Step[200/352], Loss: 1.1873\n",
      "Epoch [8/10], Step[300/352], Loss: 1.1016\n",
      "Epoch [8/10], Step[352/352], Loss: 1.0502\n",
      "Epoch[8]: v_loss: 1.13133 v_acc: 60.51\n",
      "Epoch [9/10], Step[100/352], Loss: 0.9879\n",
      "Epoch [9/10], Step[200/352], Loss: 1.0031\n",
      "Epoch [9/10], Step[300/352], Loss: 0.9370\n",
      "Epoch [9/10], Step[352/352], Loss: 0.9115\n",
      "Epoch[9]: v_loss: 1.12655 v_acc: 60.39\n",
      "Epoch [10/10], Step[100/352], Loss: 0.9046\n",
      "Epoch [10/10], Step[200/352], Loss: 1.0862\n",
      "Epoch [10/10], Step[300/352], Loss: 1.0094\n",
      "Epoch [10/10], Step[352/352], Loss: 1.1160\n",
      "Epoch[10]: v_loss: 1.12287 v_acc: 60.76\n",
      "Best model saved at epoch:  10\n",
      "Best acc model saved at epoch:  10\n",
      "Accuracy of the network on the 10000 test images: 60.76 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "185742.19814912"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define your model (assuming GeneralEEModel is defined elsewhere)\n",
    "# model = GeneralEEModel().to(device)\n",
    "model = Baseline().to(device)\n",
    "model_size, params = print_full_model(model)\n",
    "print(\"Baseline-S Model Size:\", model_size, \"KB\")\n",
    "print(\"Baseline-S Model Params:\", params)\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "dataset = CIFAR10(root='./data', download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root='./data', train=False, transform=ToTensor())\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 128\n",
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, num_workers=4)\n",
    "\n",
    "# Define other parameters\n",
    "target_size = 256  # Target size in KB\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "regularizerParam = 0.0\n",
    "compressionStep = 0.1\n",
    "initialCompressionStep = 0.1\n",
    "fastCompression = False\n",
    "modelName = \"_256KB\"\n",
    "device = \"cpu\"\n",
    "accuracyAware = True\n",
    "layersFactorization = True\n",
    "calculateInputs = None\n",
    "\n",
    "# Call the compress_NN_models function\n",
    "compress_NN_models(\n",
    "    model, target_size, train_loader, test_loader,\n",
    "    val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate,\n",
    "    criterion=criterion, regularizerParam=regularizerParam, compressionStep=compressionStep,\n",
    "    initialCompressionStep=initialCompressionStep, fastCompression=fastCompression,\n",
    "    modelName=modelName, device=device, accuracyAware=accuracyAware,\n",
    "    layersFactorization=layersFactorization, calculateInputs=calculateInputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline-S Model Size: 718.376 KB\n",
      "Baseline-S Model Params: 179594\n",
      "Directory '_256KB_C_code' created\n",
      "Directory '_256KB_C_code/headers' created\n",
      "Values: 0 0\n",
      "\n",
      "Iterating over the module:\n",
      "\n",
      "Weight shape: torch.Size([64, 3, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 64, 30, 30])\n",
      "Layer conv 1 Values: 1 1\n",
      "Shape output of the module: torch.Size([1, 64, 15, 15])\n",
      "Output Dimension: 4\n",
      "Layer pooling 1 Values: 2 2\n",
      "Weight shape: torch.Size([128, 64, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 128, 13, 13])\n",
      "Layer conv 2 Values: 3 3\n",
      "Shape output of the module: torch.Size([1, 128, 6, 6])\n",
      "Output Dimension: 4\n",
      "Layer pooling 2 Values: 4 4\n",
      "Weight shape: torch.Size([64, 128, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 64, 4, 4])\n",
      "Layer conv 3 Values: 5 5\n",
      "Shape output of the module: torch.Size([1, 64, 2, 2])\n",
      "Output Dimension: 4\n",
      "Layer pooling 3 Values: 6 6\n",
      "Shape output of the module: torch.Size([1, 256])\n",
      "Layer flatten 1 Values: 7 7\n",
      "Weight shape: torch.Size([25, 256])\n",
      "Shape output of the module: torch.Size([1, 25])\n",
      "Layer fc 1 Values: 8 8\n",
      "Weight shape: torch.Size([256, 25])\n",
      "Shape output of the module: torch.Size([1, 256])\n",
      "Layer fc 2 Values: 9 9\n",
      "Weight shape: torch.Size([64, 256])\n",
      "Shape output of the module: torch.Size([1, 64])\n",
      "Layer fc 3 Values: 10 10\n",
      "Weight shape: torch.Size([10, 64])\n",
      "Shape output of the module: torch.Size([1, 10])\n",
      "Layer fc 4 Values: 11 11\n",
      "\n",
      "Checked 11 / 11 layers\n",
      "Saved 11 / 11 layers\n",
      "\n",
      "Ignored these modules:\n",
      "[]\n",
      "\n",
      "\n",
      "Finished Saving the Model\n"
     ]
    }
   ],
   "source": [
    "model = Baseline_compressed().to(device)\n",
    "# print('Expected model keys: \\n',model.state_dict().keys())  # Expected keys\n",
    "# print('loaded model keys: \\n',torch.load(\"compressed_model_186.h5\").keys())  # Loaded keys\n",
    "model.load_state_dict(torch.load(\"/home/mal/DScale/momin_flex_nns/freeml/FreeML/SparseComp/_256KB_186.h5\", map_location='cpu'))\n",
    "model_size, params = print_full_model(model)\n",
    "print(\"Baseline-S Model Size:\", model_size, \"KB\")\n",
    "print(\"Baseline-S Model Params:\", params)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Download dataset and get a single sample\n",
    "single_sample, _ = dataset[0]  # Extract first sample (image, label)\n",
    "\n",
    "# Add batch dimension\n",
    "single_sample = single_sample.unsqueeze(0)  # Shape: (1, 3, 32, 32)\n",
    "\n",
    "save_compressed_model(model, 'csr', input_data=single_sample, directory='_256KB_C_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 30, 30]           1,792\n",
      "         MaxPool2d-2           [-1, 64, 15, 15]               0\n",
      "            Conv2d-3          [-1, 128, 13, 13]          73,856\n",
      "         MaxPool2d-4            [-1, 128, 6, 6]               0\n",
      "            Conv2d-5             [-1, 64, 4, 4]          73,792\n",
      "         MaxPool2d-6             [-1, 64, 2, 2]               0\n",
      "           Flatten-7                  [-1, 256]               0\n",
      "            Linear-8                   [-1, 25]           6,400\n",
      "            Linear-9                  [-1, 256]           6,656\n",
      "           Linear-10                   [-1, 64]          16,448\n",
      "           Linear-11                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 179,594\n",
      "Trainable params: 179,594\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.76\n",
      "Params size (MB): 0.69\n",
      "Estimated Total Size (MB): 1.46\n",
      "----------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "Actual active weights per layer\n",
      "\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "conv1 1728\n",
      "conv2 3527\n",
      "conv3 3527\n",
      "fc1.0 6400\n",
      "fc1.1 6400\n",
      "fc2 16384\n",
      "fc3 640\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = Baseline_compressed().to(device)\n",
    "# print('Expected model keys: \\n',model.state_dict().keys())  # Expected keys\n",
    "# print('loaded model keys: \\n',torch.load(\"compressed_model_186.h5\").keys())  # Loaded keys\n",
    "model.load_state_dict(torch.load(\"/home/mal/DScale/momin_flex_nns/freeml/FreeML/SparseComp/models/256KB/_256KB_186.h5\", map_location='cpu'))\n",
    "summary(model, (3, 32, 32))\n",
    "print(100*'-','\\n')\n",
    "print(\"Actual active weights per layer\\n\")\n",
    "print(100*'-','\\n')\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, (torch.nn.Conv2d, torch.nn.Linear)):\n",
    "        print(name, layer.weight.data.nonzero().size(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_flex_vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
