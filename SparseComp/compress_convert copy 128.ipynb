{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Compress Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sparsecomp import compress_NN_models\n",
    "import torch\n",
    "from conversion import save_compressed_model\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"GPU-949091da-1455-7238-7234-08221ff71c62\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_model(model):\n",
    "    assert isinstance(model, nn.Module), \"The model is not a subclass of torch.nn.Module\"\n",
    "    kb = 1000\n",
    "    model_size = 0\n",
    "    params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_size = param.nelement() * param.element_size()\n",
    "        model_size += layer_size\n",
    "        # print(name,\"\\t\", param.nelement(), \"\\t\", param.element_size(),\"\\t\", layer_size / kb, \"KB\")\n",
    "\n",
    "    for name, buffer in model.named_buffers():\n",
    "        layer_size = buffer.nelement() * buffer.element_size()\n",
    "        model_size += layer_size\n",
    "        # print(name,\"\\t\", layer_size / kb, \"KB\")\n",
    "    # print(\"Model Size:\", model_size / kb, \"KB\")\n",
    "\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    # print(\"Model Params:\", params)\n",
    "\n",
    "    return (model_size / kb), params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline_compressed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline_compressed, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"valid\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=25, bias=False),\n",
    "            nn.Linear(in_features=25, out_features=256)\n",
    "        )\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        exit_outputs = []\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        exit_outputs.append(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1[0](x)\n",
    "        x = self.fc1[1](x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x, exit_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"valid\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=\"valid\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        exit_outputs = []\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        exit_outputs.append(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        exit_outputs.append(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x, exit_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for S_earlyexit_model\n",
    "class GeneralEEModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneralEEModel, self).__init__()\n",
    "        self.pool_kernels = [\n",
    "            (1, 6, 6), (1, 3, 3), (1, 1, 1)\n",
    "        ]\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc = nn.Linear(in_features=1024, out_features=10)\n",
    "    \n",
    "    def forward(self, x, inference=False):\n",
    "        pooled_outs = []\n",
    "        for layer, out in enumerate(x):\n",
    "            pool_3d = nn.MaxPool3d(kernel_size=self.pool_kernels[layer])\n",
    "            pooled_outs.append(pool_3d(x))\n",
    "        x = torch.cat(pooled_outs, dim=1)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        scores = self.fc(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline-S Model Size: 929.32 KB\n",
      "Baseline-S Model Params: 232330\n",
      "Files already downloaded and verified\n",
      "Target size requested: 128 KB\n",
      "Iteration: 0\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 128, 13, 13]) 86528 295424 295424\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv2 with memory usage: 288.500 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 1\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 64, 4, 4]) 4096 295168 295168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: conv3 with memory usage: 288.250 KB\n",
      "transpose() received an invalid combination of arguments - got (list), but expected one of:\n",
      " * (int dim0, int dim1)\n",
      " * (name dim0, name dim1)\n",
      "\n",
      "None\n",
      "Iteration: 2\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 256]) 1024 263168 263168\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc1 with memory usage: 257.000 KB\n",
      "Sequential(\n",
      "  (0): Linear(in_features=256, out_features=25, bias=False)\n",
      "  (1): Linear(in_features=25, out_features=256, bias=True)\n",
      ")\n",
      "Iteration: 3\n",
      "torch.Size([1, 64, 30, 30]) 230400 7168 7168\n",
      "torch.Size([1, 25]) 100 25600 25600\n",
      "torch.Size([1, 256]) 1024 26624 26624\n",
      "torch.Size([1, 64]) 256 65792 65792\n",
      "torch.Size([1, 10]) 40 2600 2600\n",
      "Largest layer: fc2 with memory usage: 64.250 KB\n",
      "Layer already small 65792\n",
      "-------------- FINISHED FACTORIZATION --------------\n",
      "Test accuracy: 10.0\n",
      "----------------------------------------------------\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 1728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Conv2d, Sparsity: 1.0000, Total Elements: 73728, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 6400, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 16384, Zero Elements: 0\n",
      "Layer: Linear, Sparsity: 1.0000, Total Elements: 640, Zero Elements: 0\n",
      "Starting Density of model's parameters: [1, 1, 1, 1, 1, 1, 1]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Layer 1 Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 1:\t\t 7.168 KB, \tweight:\t 6.912 KB, \tbias: 0.256 KB\n",
      "Layer 2 Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 2:\t\t 295.424 KB, \tweight:\t 294.912 KB, \tbias: 0.512 KB\n",
      "Layer 3 Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Layer 3:\t\t 295.168 KB, \tweight:\t 294.912 KB, \tbias: 0.256 KB\n",
      "Layer 4 Linear(in_features=256, out_features=25, bias=False)\n",
      "Layer 4:\t\t 25.6 KB, \tweight:\t 25.6 KB, \tbias: 0.0 KB\n",
      "Layer 5 Linear(in_features=25, out_features=256, bias=True)\n",
      "Layer 5:\t\t 26.624 KB, \tweight:\t 25.6 KB, \tbias: 1.024 KB\n",
      "Layer 6 Linear(in_features=256, out_features=64, bias=True)\n",
      "Layer 6:\t\t 65.792 KB, \tweight:\t 65.536 KB, \tbias: 0.256 KB\n",
      "Layer 7 Linear(in_features=64, out_features=10, bias=True)\n",
      "Layer 7:\t\t 2.6 KB, \tweight:\t 2.56 KB, \tbias: 0.04 KB\n",
      "Size FC Layer (no sparsity):\t 718.376 KB\n",
      "Size FC Layer (with sparsity):\t 718.376 KB\n",
      "Total Size no sparsity:\t\t 718.376 KB\n",
      "Total Size with sparsity:\t 718.376 KB\n",
      "Total Size with sparsity and CSC representation:\t 718.376 KB\n",
      "-------------------------------------------------------------------------------------------\n",
      "uncomp: 718.376 KB\n",
      "comp: 718.376 KB\n",
      "Starting size of the model: 718.376 KB\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data22/mal/.pyenv/versions/env_flex_vit/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [256, 25], strides() = [1, 256]\n",
      "param.sizes() = [256, 25], strides() = [25, 1] (Triggered internally at ../torch/csrc/autograd/functions/accumulate_grad.h:202.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step[100/352], Loss: 2.0473\n",
      "Epoch [1/10], Step[200/352], Loss: 1.8887\n",
      "Epoch [1/10], Step[300/352], Loss: 1.7980\n",
      "Epoch [1/10], Step[352/352], Loss: 1.9161\n",
      "Epoch[1]: v_loss: 1.63187 v_acc: 38.69\n",
      "Epoch [2/10], Step[100/352], Loss: 1.5817\n",
      "Epoch [2/10], Step[200/352], Loss: 1.4978\n",
      "Epoch [2/10], Step[300/352], Loss: 1.4141\n",
      "Epoch [2/10], Step[352/352], Loss: 1.4392\n",
      "Epoch[2]: v_loss: 1.42738 v_acc: 46.61\n",
      "Epoch [3/10], Step[100/352], Loss: 1.2886\n",
      "Epoch [3/10], Step[200/352], Loss: 1.4595\n",
      "Epoch [3/10], Step[300/352], Loss: 1.3127\n",
      "Epoch [3/10], Step[352/352], Loss: 1.3979\n",
      "Epoch[3]: v_loss: 1.37203 v_acc: 49.34\n",
      "Epoch [4/10], Step[100/352], Loss: 1.1620\n",
      "Epoch [4/10], Step[200/352], Loss: 1.2718\n",
      "Epoch [4/10], Step[300/352], Loss: 1.3809\n",
      "Epoch [4/10], Step[352/352], Loss: 1.6622\n",
      "Epoch[4]: v_loss: 1.28838 v_acc: 53.27\n",
      "Epoch [5/10], Step[100/352], Loss: 1.3305\n",
      "Epoch [5/10], Step[200/352], Loss: 1.0964\n",
      "Epoch [5/10], Step[300/352], Loss: 1.1740\n",
      "Epoch [5/10], Step[352/352], Loss: 0.9919\n",
      "Epoch[5]: v_loss: 1.21338 v_acc: 56.28\n",
      "Epoch [6/10], Step[100/352], Loss: 1.0668\n",
      "Epoch [6/10], Step[200/352], Loss: 1.0974\n",
      "Epoch [6/10], Step[300/352], Loss: 1.0341\n",
      "Epoch [6/10], Step[352/352], Loss: 1.1075\n",
      "Epoch[6]: v_loss: 1.17183 v_acc: 58.08\n",
      "Epoch [7/10], Step[100/352], Loss: 1.2094\n",
      "Epoch [7/10], Step[200/352], Loss: 1.1075\n",
      "Epoch [7/10], Step[300/352], Loss: 0.9870\n",
      "Epoch [7/10], Step[352/352], Loss: 1.1408\n",
      "Epoch[7]: v_loss: 1.18659 v_acc: 57.54\n",
      "Epoch [8/10], Step[100/352], Loss: 0.9775\n",
      "Epoch [8/10], Step[200/352], Loss: 1.1798\n",
      "Epoch [8/10], Step[300/352], Loss: 1.0353\n",
      "Epoch [8/10], Step[352/352], Loss: 1.0772\n",
      "Epoch[8]: v_loss: 1.18421 v_acc: 58.14\n",
      "Epoch [9/10], Step[100/352], Loss: 1.0700\n",
      "Epoch [9/10], Step[200/352], Loss: 0.9216\n",
      "Epoch [9/10], Step[300/352], Loss: 1.0674\n",
      "Epoch [9/10], Step[352/352], Loss: 1.0702\n",
      "Epoch[9]: v_loss: 1.11879 v_acc: 60.85\n",
      "Epoch [10/10], Step[100/352], Loss: 1.0045\n",
      "Epoch [10/10], Step[200/352], Loss: 1.0447\n",
      "Epoch [10/10], Step[300/352], Loss: 1.0206\n",
      "Epoch [10/10], Step[352/352], Loss: 0.7988\n",
      "Epoch[10]: v_loss: 1.12587 v_acc: 60.23\n",
      "Best model saved at epoch:  9\n",
      "Best acc model saved at epoch:  9\n",
      "Accuracy of the network on the 10000 test images: 60.85 %\n",
      "0 iteration -  Size: 452187.09907456 [1, 0.04782969, 1, 1, 1, 1, 1]\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n",
      "Epoch [1/10], Step[100/352], Loss: 1.0307\n",
      "Epoch [1/10], Step[200/352], Loss: 1.1249\n",
      "Epoch [1/10], Step[300/352], Loss: 1.1310\n",
      "Epoch [1/10], Step[352/352], Loss: 1.1071\n",
      "Epoch[1]: v_loss: 1.1322 v_acc: 59.91\n",
      "Epoch [2/10], Step[100/352], Loss: 0.9493\n",
      "Epoch [2/10], Step[200/352], Loss: 1.1270\n",
      "Epoch [2/10], Step[300/352], Loss: 1.0596\n",
      "Epoch [2/10], Step[352/352], Loss: 1.0732\n",
      "Epoch[2]: v_loss: 1.11873 v_acc: 60.26\n",
      "Epoch [3/10], Step[100/352], Loss: 1.0807\n",
      "Epoch [3/10], Step[200/352], Loss: 0.8618\n",
      "Epoch [3/10], Step[300/352], Loss: 0.9234\n",
      "Epoch [3/10], Step[352/352], Loss: 0.8591\n",
      "Epoch[3]: v_loss: 1.13132 v_acc: 60.02\n",
      "Epoch [4/10], Step[100/352], Loss: 1.0535\n",
      "Epoch [4/10], Step[200/352], Loss: 1.1937\n",
      "Epoch [4/10], Step[300/352], Loss: 1.0483\n",
      "Epoch [4/10], Step[352/352], Loss: 1.1811\n",
      "Epoch[4]: v_loss: 1.13732 v_acc: 59.45\n",
      "Epoch [5/10], Step[100/352], Loss: 0.8980\n",
      "Epoch [5/10], Step[200/352], Loss: 0.9099\n",
      "Epoch [5/10], Step[300/352], Loss: 1.0295\n",
      "Epoch [5/10], Step[352/352], Loss: 0.8979\n",
      "Epoch[5]: v_loss: 1.09351 v_acc: 61.31\n",
      "Epoch [6/10], Step[100/352], Loss: 0.9181\n",
      "Epoch [6/10], Step[200/352], Loss: 1.0723\n",
      "Epoch [6/10], Step[300/352], Loss: 1.0602\n",
      "Epoch [6/10], Step[352/352], Loss: 0.9480\n",
      "Epoch[6]: v_loss: 1.13058 v_acc: 60.54\n",
      "Epoch [7/10], Step[100/352], Loss: 1.0852\n",
      "Epoch [7/10], Step[200/352], Loss: 0.9904\n",
      "Epoch [7/10], Step[300/352], Loss: 0.9533\n",
      "Epoch [7/10], Step[352/352], Loss: 0.8850\n",
      "Epoch[7]: v_loss: 1.13948 v_acc: 60.27\n",
      "Epoch [8/10], Step[100/352], Loss: 1.0018\n",
      "Epoch [8/10], Step[200/352], Loss: 1.0036\n",
      "Epoch [8/10], Step[300/352], Loss: 1.0319\n",
      "Epoch [8/10], Step[352/352], Loss: 1.0091\n",
      "Epoch[8]: v_loss: 1.10035 v_acc: 61.7\n",
      "Epoch [9/10], Step[100/352], Loss: 0.8834\n",
      "Epoch [9/10], Step[200/352], Loss: 1.1077\n",
      "Epoch [9/10], Step[300/352], Loss: 1.0899\n",
      "Epoch [9/10], Step[352/352], Loss: 0.9640\n",
      "Epoch[9]: v_loss: 1.10501 v_acc: 61.56\n",
      "Epoch [10/10], Step[100/352], Loss: 0.9401\n",
      "Epoch [10/10], Step[200/352], Loss: 0.8571\n",
      "Epoch [10/10], Step[300/352], Loss: 0.8376\n",
      "Epoch [10/10], Step[352/352], Loss: 0.8514\n",
      "Epoch[10]: v_loss: 1.08893 v_acc: 61.69\n",
      "Best model saved at epoch:  10\n",
      "Best acc model saved at epoch:  8\n",
      "Accuracy of the network on the 10000 test images: 61.69 %\n",
      "1 iteration -  Size: 185742.19814912 [1, 0.04782969, 0.04782969, 1, 1, 1, 1]\n",
      "Disabling: conv1.weight\n",
      "Disabling: conv1.bias\n",
      "Disabling: conv2.weight\n",
      "Disabling: conv2.bias\n",
      "Disabling: conv3.weight\n",
      "Disabling: conv3.bias\n",
      "Disabling: fc1.0.weight\n",
      "Disabling: fc1.1.weight\n",
      "Disabling: fc1.1.bias\n",
      "Disabling: fc2.weight\n",
      "Disabling: fc2.bias\n",
      "Disabling: fc3.weight\n",
      "Disabling: fc3.bias\n",
      "Activating: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
      "Activating: Linear(in_features=256, out_features=25, bias=False)\n",
      "Activating: Linear(in_features=25, out_features=256, bias=True)\n",
      "Activating: Linear(in_features=256, out_features=64, bias=True)\n",
      "Activating: Linear(in_features=64, out_features=10, bias=True)\n",
      "Epoch [1/10], Step[100/352], Loss: 1.1449\n",
      "Epoch [1/10], Step[200/352], Loss: 1.1373\n",
      "Epoch [1/10], Step[300/352], Loss: 0.9827\n",
      "Epoch [1/10], Step[352/352], Loss: 0.9512\n",
      "Epoch[1]: v_loss: 1.07254 v_acc: 62.04\n",
      "Epoch [2/10], Step[100/352], Loss: 0.8555\n",
      "Epoch [2/10], Step[200/352], Loss: 1.1211\n",
      "Epoch [2/10], Step[300/352], Loss: 1.0641\n",
      "Epoch [2/10], Step[352/352], Loss: 0.9447\n",
      "Epoch[2]: v_loss: 1.07946 v_acc: 61.99\n",
      "Epoch [3/10], Step[100/352], Loss: 0.9204\n",
      "Epoch [3/10], Step[200/352], Loss: 0.9074\n",
      "Epoch [3/10], Step[300/352], Loss: 0.8835\n",
      "Epoch [3/10], Step[352/352], Loss: 0.9517\n",
      "Epoch[3]: v_loss: 1.0805 v_acc: 61.7\n",
      "Epoch [4/10], Step[100/352], Loss: 1.0768\n",
      "Epoch [4/10], Step[200/352], Loss: 0.9215\n",
      "Epoch [4/10], Step[300/352], Loss: 0.8224\n",
      "Epoch [4/10], Step[352/352], Loss: 1.0867\n",
      "Epoch[4]: v_loss: 1.12764 v_acc: 60.83\n",
      "Epoch [5/10], Step[100/352], Loss: 0.9626\n",
      "Epoch [5/10], Step[200/352], Loss: 0.8928\n",
      "Epoch [5/10], Step[300/352], Loss: 1.0957\n",
      "Epoch [5/10], Step[352/352], Loss: 0.8127\n",
      "Epoch[5]: v_loss: 1.09245 v_acc: 62.26\n",
      "Epoch [6/10], Step[100/352], Loss: 0.9792\n",
      "Epoch [6/10], Step[200/352], Loss: 1.0126\n",
      "Epoch [6/10], Step[300/352], Loss: 1.1272\n",
      "Epoch [6/10], Step[352/352], Loss: 1.0007\n",
      "Epoch[6]: v_loss: 1.08166 v_acc: 62.25\n",
      "Epoch [7/10], Step[100/352], Loss: 0.9483\n",
      "Epoch [7/10], Step[200/352], Loss: 0.8437\n",
      "Epoch [7/10], Step[300/352], Loss: 1.0748\n",
      "Epoch [7/10], Step[352/352], Loss: 0.8772\n",
      "Epoch[7]: v_loss: 1.0819 v_acc: 62.03\n",
      "Epoch [8/10], Step[100/352], Loss: 1.0088\n",
      "Epoch [8/10], Step[200/352], Loss: 0.9451\n",
      "Epoch [8/10], Step[300/352], Loss: 0.8539\n",
      "Epoch [8/10], Step[352/352], Loss: 0.7383\n",
      "Epoch[8]: v_loss: 1.11496 v_acc: 61.64\n",
      "Epoch [9/10], Step[100/352], Loss: 0.9064\n",
      "Epoch [9/10], Step[200/352], Loss: 1.0140\n",
      "Epoch [9/10], Step[300/352], Loss: 0.9602\n",
      "Epoch [9/10], Step[352/352], Loss: 1.0229\n",
      "Epoch[9]: v_loss: 1.08966 v_acc: 62.0\n",
      "Epoch [10/10], Step[100/352], Loss: 0.8604\n",
      "Epoch [10/10], Step[200/352], Loss: 0.9711\n",
      "Epoch [10/10], Step[300/352], Loss: 0.9692\n",
      "Epoch [10/10], Step[352/352], Loss: 0.8806\n",
      "Epoch[10]: v_loss: 1.07676 v_acc: 63.03\n",
      "Best model saved at epoch:  1\n",
      "Best acc model saved at epoch:  10\n",
      "Accuracy of the network on the 10000 test images: 62.04 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "127503.3312768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define your model (assuming GeneralEEModel is defined elsewhere)\n",
    "# model = GeneralEEModel().to(device)\n",
    "model = Baseline().to(device)\n",
    "model_size, params = print_full_model(model)\n",
    "print(\"Baseline-S Model Size:\", model_size, \"KB\")\n",
    "print(\"Baseline-S Model Params:\", params)\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "dataset = CIFAR10(root='./data', download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root='./data', train=False, transform=ToTensor())\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 128\n",
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, num_workers=4)\n",
    "\n",
    "# Define other parameters\n",
    "target_size = 128  # Target size in KB\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "regularizerParam = 0.0\n",
    "compressionStep = 0.1\n",
    "initialCompressionStep = 0.1\n",
    "fastCompression = False\n",
    "modelName = \"_128KB\"\n",
    "device = \"cpu\"\n",
    "accuracyAware = True\n",
    "layersFactorization = True\n",
    "calculateInputs = None\n",
    "\n",
    "# Call the compress_NN_models function\n",
    "compress_NN_models(\n",
    "    model, target_size, train_loader, test_loader,\n",
    "    val_loader=val_loader, num_epochs=num_epochs, learning_rate=learning_rate,\n",
    "    criterion=criterion, regularizerParam=regularizerParam, compressionStep=compressionStep,\n",
    "    initialCompressionStep=initialCompressionStep, fastCompression=fastCompression,\n",
    "    modelName=modelName, device=device, accuracyAware=accuracyAware,\n",
    "    layersFactorization=layersFactorization, calculateInputs=calculateInputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline-S Model Size: 718.376 KB\n",
      "Baseline-S Model Params: 179594\n",
      "Directory '_128KB_C_code' created\n",
      "Directory '_128KB_C_code/headers' created\n",
      "Values: 0 0\n",
      "\n",
      "Iterating over the module:\n",
      "\n",
      "Weight shape: torch.Size([64, 3, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 64, 30, 30])\n",
      "Layer conv 1 Values: 1 1\n",
      "Shape output of the module: torch.Size([1, 64, 15, 15])\n",
      "Output Dimension: 4\n",
      "Layer pooling 1 Values: 2 2\n",
      "Weight shape: torch.Size([128, 64, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 128, 13, 13])\n",
      "Layer conv 2 Values: 3 3\n",
      "Shape output of the module: torch.Size([1, 128, 6, 6])\n",
      "Output Dimension: 4\n",
      "Layer pooling 2 Values: 4 4\n",
      "Weight shape: torch.Size([64, 128, 3, 3])\n",
      "Shape output of the module: torch.Size([1, 64, 4, 4])\n",
      "Layer conv 3 Values: 5 5\n",
      "Shape output of the module: torch.Size([1, 64, 2, 2])\n",
      "Output Dimension: 4\n",
      "Layer pooling 3 Values: 6 6\n",
      "Shape output of the module: torch.Size([1, 256])\n",
      "Layer flatten 1 Values: 7 7\n",
      "Weight shape: torch.Size([25, 256])\n",
      "Shape output of the module: torch.Size([1, 25])\n",
      "Layer fc 1 Values: 8 8\n",
      "Weight shape: torch.Size([256, 25])\n",
      "Shape output of the module: torch.Size([1, 256])\n",
      "Layer fc 2 Values: 9 9\n",
      "Weight shape: torch.Size([64, 256])\n",
      "Shape output of the module: torch.Size([1, 64])\n",
      "Layer fc 3 Values: 10 10\n",
      "Weight shape: torch.Size([10, 64])\n",
      "Shape output of the module: torch.Size([1, 10])\n",
      "Layer fc 4 Values: 11 11\n",
      "\n",
      "Checked 11 / 11 layers\n",
      "Saved 11 / 11 layers\n",
      "\n",
      "Ignored these modules:\n",
      "[]\n",
      "\n",
      "\n",
      "Finished Saving the Model\n"
     ]
    }
   ],
   "source": [
    "model = Baseline_compressed().to(device)\n",
    "# print('Expected model keys: \\n',model.state_dict().keys())  # Expected keys\n",
    "# print('loaded model keys: \\n',torch.load(\"compressed_model_186.h5\").keys())  # Loaded keys\n",
    "model.load_state_dict(torch.load(\"/home/mal/DScale/momin_flex_nns/freeml/FreeML/SparseComp/_128KB_128.h5\", map_location='cpu'))\n",
    "model_size, params = print_full_model(model)\n",
    "print(\"Baseline-S Model Size:\", model_size, \"KB\")\n",
    "print(\"Baseline-S Model Params:\", params)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Download dataset and get a single sample\n",
    "single_sample, _ = dataset[0]  # Extract first sample (image, label)\n",
    "\n",
    "# Add batch dimension\n",
    "single_sample = single_sample.unsqueeze(0)  # Shape: (1, 3, 32, 32)\n",
    "\n",
    "save_compressed_model(model, 'csr', input_data=single_sample, directory='_128KB_C_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 30, 30]           1,792\n",
      "         MaxPool2d-2           [-1, 64, 15, 15]               0\n",
      "            Conv2d-3          [-1, 128, 13, 13]          73,856\n",
      "         MaxPool2d-4            [-1, 128, 6, 6]               0\n",
      "            Conv2d-5             [-1, 64, 4, 4]          73,792\n",
      "         MaxPool2d-6             [-1, 64, 2, 2]               0\n",
      "           Flatten-7                  [-1, 256]               0\n",
      "            Linear-8                   [-1, 25]           6,400\n",
      "            Linear-9                  [-1, 256]           6,656\n",
      "           Linear-10                   [-1, 64]          16,448\n",
      "           Linear-11                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 179,594\n",
      "Trainable params: 179,594\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.76\n",
      "Params size (MB): 0.69\n",
      "Estimated Total Size (MB): 1.46\n",
      "----------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "Actual active weights per layer\n",
      "\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "conv1 1728\n",
      "conv2 3527\n",
      "conv3 3527\n",
      "fc1.0 6400\n",
      "fc1.1 6400\n",
      "fc2 784\n",
      "fc3 640\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = Baseline_compressed().to(device)\n",
    "# print('Expected model keys: \\n',model.state_dict().keys())  # Expected keys\n",
    "# print('loaded model keys: \\n',torch.load(\"compressed_model_186.h5\").keys())  # Loaded keys\n",
    "model.load_state_dict(torch.load(\"/home/mal/DScale/momin_flex_nns/freeml/FreeML/SparseComp/models/128KB/_128KB_128.h5\", map_location='cpu'))\n",
    "summary(model, (3, 32, 32))\n",
    "print(100*'-','\\n')\n",
    "print(\"Actual active weights per layer\\n\")\n",
    "print(100*'-','\\n')\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, (torch.nn.Conv2d, torch.nn.Linear)):\n",
    "        print(name, layer.weight.data.nonzero().size(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_flex_vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
